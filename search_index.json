[
["index.html", "统计笔记 1 前言", " 统计笔记 余光创 2016-04-26   1 前言  ____  _        _   _     _             _   _       _ / ___|| |_ __ _| |_(_)___| |_ ___ ___  | \\ | | ___ | |_ ___  ___ \\___ \\| __/ _` | __| / __| __/ __/ __| |  \\| |/ _ \\| __/ _ \\/ __|  ___) | || (_| | |_| \\__ \\ || (__\\__ \\ | |\\  | (_) | ||  __/\\__ \\ |____/ \\__\\__,_|\\__|_|___/\\__\\___|___/ |_| \\_|\\___/ \\__\\___||___/  若想了解上帝在想什么，我们就必须学统计，因为统计学就是在量测他的旨意。                                                       ——南丁格尔 在暨大工作几年，发现生物学生大多不懂最基础的统计，而这是一个非常普遍的现象，生物坑对t检验特别有爱，我甚至看到用T检验去分析免疫组化数据的，而且还发在Nature子刊上，我都吐槽无力了。在labmeeting学生经常被小老板喷乱用T检验，我跟很多学生讲t检验的最基本的核心概念，standard error of the mean。后来在2013年初开始用markdown写一些笔记，本意是给暨南大学功能蛋白研究中心的学生们看的，然而似乎并没有什么用，大多数人都是从师兄师姐那里拿一个excel表格，把自己的数据套进去用t检验，至于行不行对不对，似乎很多人也不太关心，除非被老板骂了。  "],
["r.html", "2 R语言简介 2.1 控制流 （Flow Control) 2.2 循环 (Looping)", " 2 R语言简介 关于编程，Jeremy Penzer写了一个guidlines，将其摘录如下： &gt; - Problem specification. This is the starting point for any program. Just as you would develop an objective and make an outline if you had to write a paper or report, rather than simply start writing, you should specify the problem as clearly as possible. Particularly important is determining what inputs your program needs and what outputs it should produce. It is also important to consider the problem and its solution from the perspective of those who will use the program and those who will benefit from the solution.   Code planning. Contrary to what may be your notion of where to start, it is best not to start writing code, but rather to sketch out a rough version of the program with a pen and paper (or on a computer screen). If you know how to make a flow chart, that is a great idea too. Plan the code by making a nontechnical program prototype. Mock up the desired format of the output.     Identify constants. If you plan to use a constant or a variable, it is good to give that value an identifier at the beginning of the program. The advantage is obvious. If you assign a value to an argument or a constant, you can change that value once, and update all instances throughout the program.     Program documentation. Good programs are self-documenting. Lay out the code logically and clearly. Use spacing and indentation to make the code readable, and use comments to describe the complicated parts of your program. This helps you and others. When you comment the more complicated parts of your code, you will not have to remember weeks later what was very simple and obvious to you at the time, and you can sometimes spend hours deciphering your own “once-obvious” code. This is the voice of experience speaking here.     Solve runtime problems. Although books on programming are usually linear and straightforward, programming in real life is not like that at all. You may break your problem down into several steps, and work on them at different times. You will almost always find that your code does not produce the expected or desired results when you first execute it. You may or may not get much feedback about what went wrong.     Verify your program. As we are not developing large or sophisticated programs or functions, we are usually satisfied simply when we use some test cases for which we know the correct answers. More advanced forms of program verification are not needed in this case.    2.1 控制流 （Flow Control)   2.2 循环 (Looping)   "],
["ggplot2.html", "3 使用ggplot2画图 3.1 Why use ggplot2 3.2 ggplot2基本要素 3.3 数据（Data）和映射（Mapping) 3.4 几何对象（Geometric） 3.5 标尺（Scale） 3.6 统计变换（Statistics） 3.7 坐标系统（Coordinante） 3.8 图层（Layer） 3.9 分面（Facet） 3.10 主题（Theme） 3.11 二维密度图 3.12 ggplot2实战", " 3 使用ggplot2画图  3.1 Why use ggplot2 ggplot2是我见过最human friendly的画图软件，这得益于Leland Wilkinson在他的著作《The Grammar of Graphics》中提出了一套图形语法，把图形元素抽象成可以自由组合的成分，Hadley Wickham把这套想法在R中实现。 为什么要学习ggplot2，可以参考ggplot2: 数据分析与图形艺术的序言（btw: 在序言的最后，我被致谢了）。 Hadley Wickham也给出一堆理由让我们说服自己，我想再补充一点，Hadley Wickham是学医出身的，做为学生物出身的人有什么理由不支持呢:)   3.2 ggplot2基本要素  数据（Data）和映射（Mapping） 几何对象（Geometric） 标尺（Scale） 统计变换（Statistics） 坐标系统（Coordinante） 图层（Layer） 分面（Facet） 主题（Theme）  这里将从这些基本要素对ggplot2进行介绍。   3.3 数据（Data）和映射（Mapping) 下面以一份钻石的数据为例，这份数据非常大，随机取一个子集来画图。 require(ggplot2) ## Loading required package: ggplot2 data(diamonds) set.seed(42) small&lt;-diamonds[sample(nrow(diamonds),1000),]  head(small) ##       carat       cut color clarity depth table price    x    y    z ## 49345  0.71 Very Good     H     SI1  62.5    60  2096 5.68 5.75 3.57 ## 50545  0.79   Premium     H     SI1  61.8    59  2275 5.97 5.91 3.67 ## 15434  1.03     Ideal     F     SI1  62.4    57  6178 6.48 6.44 4.03 ## 44792  0.50     Ideal     E     VS2  62.2    54  1624 5.08 5.11 3.17 ## 34614  0.27     Ideal     E     VS1  61.6    56   470 4.14 4.17 2.56 ## 27998  0.30   Premium     E     VS2  61.7    58   658 4.32 4.34 2.67 summary(small) ##      carat               cut      color      clarity        depth       ##  Min.   :0.2200   Fair     : 28   D:121   SI1    :258   Min.   :55.20   ##  1st Qu.:0.4000   Good     : 88   E:186   VS2    :231   1st Qu.:61.00   ##  Median :0.7100   Very Good:227   F:164   SI2    :175   Median :61.80   ##  Mean   :0.8187   Premium  :257   G:216   VS1    :141   Mean   :61.71   ##  3rd Qu.:1.0700   Ideal    :400   H:154   VVS2   : 91   3rd Qu.:62.50   ##  Max.   :2.6600                   I:106   VVS1   : 67   Max.   :72.20   ##                                   J: 53   (Other): 37                   ##      table           price               x               y         ##  Min.   :50.10   Min.   :  342.0   Min.   :3.850   Min.   :3.840   ##  1st Qu.:56.00   1st Qu.:  989.5   1st Qu.:4.740   1st Qu.:4.758   ##  Median :57.00   Median : 2595.0   Median :5.750   Median :5.775   ##  Mean   :57.43   Mean   : 4110.5   Mean   :5.787   Mean   :5.791   ##  3rd Qu.:59.00   3rd Qu.: 5495.2   3rd Qu.:6.600   3rd Qu.:6.610   ##  Max.   :65.00   Max.   :18795.0   Max.   :8.830   Max.   :8.870   ##                                                                    ##        z         ##  Min.   :2.330   ##  1st Qu.:2.920   ##  Median :3.550   ##  Mean   :3.572   ##  3rd Qu.:4.070   ##  Max.   :5.580   ##  画图实际上是把数据中的变量映射到图形属性上。以克拉(carat)数为X轴变量，价格(price)为Y轴变量。 p &lt;- ggplot(data=small, mapping=aes(x=carat, y=price)) 上面这行代码把数据映射XY坐标轴上，需要告诉ggplot2，这些数据要映射成什么样的几何对象，下面以散点为例： p+geom_point()  几何对象将在下面的小节介绍，这一节，关注的是数据和图形属性之间的映射。 如果想将切工（cut）映射到形状属性。只需要： p &lt;- ggplot(data=small, mapping=aes(x=carat, y=price, shape=cut)) p+geom_point()  再比如我想将钻石的颜色（color）映射颜色属性： p &lt;- ggplot(data=small, mapping=aes(x=carat, y=price, shape=cut, colour=color)) p+geom_point()    3.4 几何对象（Geometric） 在上面的例子中，各种属性映射由ggplot函数执行，只需要加一个图层，使用geom_point()告诉ggplot要画散点，于是所有的属性都映射到散点上。 geom_point()完成的就是几何对象的映射，ggplot2提供了各种几何对象映射，如geom_histogram用于直方图，geom_bar用于画柱状图，geom_boxplot用于画箱式图等等。 不同的几何对象，要求的属性会有些不同，这些属性也可以在几何对象映射时提供，比如上一图，也可以用以下语法来画： p &lt;- ggplot(small) p+geom_point(aes(x=carat, y=price, shape=cut, colour=color)) ggplot2支持图层，我通常把不同的图层中共用的映射提供给ggplot函数，而某一几何对象才需要的映射参数提供给geom_xxx函数。 这一小节我们来看一下各种常用的几何对象。  3.4.1 直方图 直方图最容易，提供一个x变量，画出数据的分布。 ggplot(small)+geom_histogram(aes(x=price)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.  同样可以根据另外的变量给它填充颜色，比如按不同的切工： ggplot(small)+geom_histogram(aes(x=price, fill=cut)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.  也可以将其分开，side-by-side地画直方图。 ggplot(small)+geom_histogram(aes(x=price, fill=cut), position=&quot;dodge&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.  还可以使用position=“fill”，按照相对比例来画。 ggplot(small)+geom_histogram(aes(x=price, fill=cut), position=&quot;fill&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.  ### 柱状图 柱状图非常适合于画分类变量。在这里以透明度（clarity）变量为例。按照不同透明度的钻石的数目画柱状图。 ggplot(small)+geom_bar(aes(x=clarity))  柱状图两个要素，一个是分类变量，一个是数目，也就是柱子的高度。数目在这里不用提供，因为ggplot2会通过x变量计算各个分类的数目。 当然你想提供也是可以的，通过stat参数，可以让geom_bar按指定高度画图，比如以下代码： ggplot()+geom_bar(aes(x=c(LETTERS[1:3]),y=1:3), stat=&quot;identity&quot;)  柱状图和直方图是很像的，直方图把连续型的数据按照一个个等长的分区（bin）来切分，然后计数，画柱状图。而柱状图是分类数据，按类别计数。我们可以用前面直方图的参数来画side-by-side的柱状图，填充颜色或者按比例画图，它们是高度一致的。 柱状图是用来表示计数数据的，但在生物界却被经常拿来表示均值，加上误差来表示数据分布，这可以通常图层来实现，我将在图层一节中给出实例。   3.4.2 密度函数图 说到直方图，就不得不说密度函数图，数据和映射和直方图是一样的，唯一不同的是几何对象，geom_histogram告诉ggplot要画直方图，而geom_density则说我们要画密度函数图，在我们熟悉前面语法的情况下，很容易画出： ggplot(small)+geom_density(aes(x=price, colour=cut))  ggplot(small)+geom_density(aes(x=price,fill=clarity))  colour参数指定的是曲线的颜色，而fill是往曲线下面填充颜色。   3.4.3 箱式图 数据量比较大的时候，用直方图和密度函数图是表示数据分布的好方法，而在数据量较少的时候，比如很多的生物实验，很多时候大家都是使用柱状图+errorbar的形式来表示，不过这种方法的信息量非常低，被Nature Methods吐槽，这种情况推荐使用boxplot。 ggplot(small)+geom_boxplot(aes(x=cut, y=price,fill=color))  geom_boxplot将数据映射到箱式图上，上面的代码，我们应该很熟悉了，按切工(cut)分类，对价格(price)变量画箱式图，再分开按照color变量填充颜色。 ggplot2提供了很多的geom_xxx函数，可以满足我们对各种图形绘制的需求。 geom_abline     geom_area    geom_bar        geom_bin2d geom_blank      geom_boxplot     geom_contour    geom_crossbar geom_density    geom_density2d   geom_dotplot    geom_errorbar geom_errorbarh  geom_freqpoly    geom_hex        geom_histogram geom_hline      geom_jitter      geom_line       geom_linerange geom_map        geom_path    geom_point      geom_pointrange geom_polygon    geom_quantile    geom_raster     geom_rect geom_ribbon     geom_rug     geom_segment    geom_smooth geom_step       geom_text    geom_tile       geom_violin geom_vline    3.5 标尺（Scale） 前面我们已经看到了，画图就是在做映射，不管是映射到不同的几何对象上，还是映射各种图形属性。这一小节介绍标尺，在对图形属性进行映射之后，使用标尺可以控制这些属性的显示方式，比如坐标刻度，可能通过标尺，将坐标进行对数变换；比如颜色属性，也可以通过标尺，进行改变。 ggplot(small)+geom_point(aes(x=carat, y=price, shape=cut, colour=color))+scale_y_log10()+scale_colour_manual(values=rainbow(7))  以数据（Data）和映射（Mapping)一节中所画散点图为例，将Y轴坐标进行log10变换，再自己定义颜色为彩虹色。   3.6 统计变换（Statistics） 统计变换对原始数据进行某种计算，然后在图上表示出来，例如对散点图上加一条回归线。 ggplot(small, aes(x=carat, y=price))+geom_point()+scale_y_log10()+stat_smooth()  这里就不按颜色、切工来分了，不然ggplot会按不同的分类变量分别做回归，图就很乱，如果我们需要这样做，我们可以使用分面，这个将在后面介绍。 这里，aes所提供的参数，就通过ggplot提供，而不是提供给geom_point，因为ggplot里的参数，相当于全局变量，geom_point()和stat_smooth()都知道x,y的映射，如果只提供给geom_point()，则相当于是局部变量，geom_point知道这种映射，而stat_smooth不知道，当然你再给stat_smooth也提供x,y的映射，不过共用的映射，还是提供给ggplot好。 ggplot2提供了多种统计变换方式： stat_abline       stat_contour      stat_identity     stat_summary stat_bin          stat_density      stat_qq           stat_summary2d stat_bin2d        stat_density2d    stat_quantile     stat_summary_hex stat_bindot       stat_ecdf         stat_smooth       stat_unique stat_binhex       stat_function     stat_spoke        stat_vline stat_boxplot      stat_hline        stat_sum          stat_ydensity 统计变换是非常重要的功能，我们可以自己写函数，基于原始数据做某种计算，并在图上表现出来，也可以通过它改变geom_xxx函数画图的默认统计参数。 比如我在Proteomic investigation of the interactome of FMNL1 in hematopoietic cells unveils a role in calcium-dependent membrane plasticity的图一中，就把boxplot的中位线替换成了平均值来作图。   3.7 坐标系统（Coordinante） 坐标系统控制坐标轴，可以进行变换，例如XY轴翻转，笛卡尔坐标和极坐标转换，以满足我们的各种需求。 坐标轴翻转由coord_flip()实现 ggplot(small)+geom_bar(aes(x=cut, fill=cut))+coord_flip()  而转换成极坐标可以由coord_polar()实现： ggplot(small)+geom_bar(aes(x=factor(1), fill=cut))+coord_polar(theta=&quot;y&quot;)  这也是为什么之前介绍常用图形画法时没有提及饼图的原因，饼图实际上就是柱状图，只不过是使用极坐标而已，柱状图的高度，对应于饼图的弧度，饼图并不推荐，因为人类的眼睛比较弧度的能力比不上比较高度（柱状图）。 还可以画靶心图： ggplot(small)+geom_bar(aes(x=factor(1), fill=cut))+coord_polar()  以及风玫瑰图(windrose) ggplot(small)+geom_bar(aes(x=clarity, fill=cut))+coord_polar()    3.8 图层（Layer） photoshop流行的原因在于PS 3.0时引入图层的概念，ggplot的牛B之处在于使用+号来叠加图层，这堪称是泛型编程的典范。 在前面散点图上，我们已经见识过，加上了一个回归线拟合的图层。 在RT-PCR统计分析中，我使用geom_bar()画了一个柱状图，再使用geom_errorbar()画了标准误，两个图层一叠加，就成了我们常画的加了errorbar的柱状图。 有了图层的概念，使用ggplot画起图来，就更加得心应手。 做为图层的一个很好的例子是蝙蝠侠logo，batman logo由6个函数组成，在下面的例子中，我先画第一个函数，之后再加一个图层画第二个函数，不断重复这一过程，直到六个函数全部画好。 require(grid) ## Loading required package: grid require(ggplot2)   f1 &lt;- function(x) {     y1 &lt;- 3*sqrt(1-(x/7)^2)     y2 &lt;- -3*sqrt(1-(x/7)^2)     y &lt;- c(y1,y2)     d &lt;- data.frame(x=x,y=y)     d &lt;- d[d$y &gt; -3*sqrt(33)/7,]     return(d) }   x1 &lt;- c(seq(3, 7, 0.001), seq(-7, -3, 0.001)) d1 &lt;- f1(x1) p1 &lt;- ggplot(d1,aes(x,y)) + geom_point(color=&quot;red&quot;) +xlab(&quot;&quot;) + ylab(&quot;&quot;) + theme_bw()   x2 &lt;- seq(-4,4, 0.001) y2 &lt;- abs(x2/2)-(3*sqrt(33)-7)*x2^2/112-3 + sqrt(1-(abs(abs(x2)-2)-1)^2) d2 &lt;- data.frame(x2=x2, y2=y2) p2 &lt;- p1 + geom_point(data=d2, aes(x=x2,y=y2), color=&quot;yellow&quot;)   x3 &lt;- c(seq(0.75,1,0.001), seq(-1,-0.75,0.001)) y3 &lt;- 9-8*abs(x3) d3 &lt;- data.frame(x3=x3, y3=y3) p3 &lt;- p2+geom_point(data=d3, aes(x=x3,y=y3), color=&quot;green&quot;)   x4 &lt;- c(seq(0.5,0.75,0.001), seq(-0.75,-0.5,0.001)) y4 &lt;- 3*abs(x4)+0.75 d4 &lt;- data.frame(x4=x4,y4=y4) p4 &lt;- p3+geom_point(data=d4, aes(x=x4,y=y4), color=&quot;steelblue&quot;)    x5 &lt;- seq(-0.5,0.5,0.001) y5 &lt;- rep(2.25,length(x5)) d5 &lt;- data.frame(x5=x5,y5=y5) p5 &lt;- p4+geom_point(data=d5, aes(x=x5,y=y5))   x6 &lt;- c(seq(-3,-1,0.001), seq(1,3,0.001)) y6 &lt;- 6 * sqrt(10)/7 +     (1.5 - 0.5 * abs(x6)) * sqrt(abs(abs(x6)-1)/(abs(x6)-1)) -     6 * sqrt(10) * sqrt(4-(abs(x6)-1)^2)/14 d6 &lt;- data.frame(x6=x6,y6=y6) p6 &lt;- p5+geom_point(data=d6,aes(x=x6,y=y6), colour=&quot;blue&quot;)  multiplot &lt;- function (..., plotlist = NULL, cols = 1, layout = NULL) {     plots &lt;- c(list(...), plotlist)     numPlots = length(plots)     if (is.null(layout)) {         layout &lt;- matrix(seq(1, cols * ceiling(numPlots/cols)),             ncol = cols, nrow = ceiling(numPlots/cols))     }     if (numPlots == 1) {         print(plots[[1]])     }     else {         grid.newpage()         pushViewport(viewport(layout = grid.layout(nrow(layout),             ncol(layout))))         for (i in 1:numPlots) {             matchidx &lt;- as.data.frame(which(layout == i, arr.ind = TRUE))             print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,                 layout.pos.col = matchidx$col))         }     } }  multiplot(p1,p2,p3,p4,p5,p6, cols=2) ## Warning: Removed 2 rows containing missing values (geom_point).    3.9 分面（Facet） 在《ggplot2: 数据分析与图形艺术》一书的翻译中，一开始译者把facet翻译成切片，我在校稿的时候发现了，给他们写信，推荐翻译成分面，如果是slice这个词，翻译成切片倒是很精准，BSD的硬盘分区就叫slice，但facet从词源上看就是小脸的意思，翻译成分面才到位。给他们写信的时候，我还专门查了CNKI翻译助手，发现这词在信息学中，翻成分面早已是固定的。我感觉这是我对这本书翻译的最大贡献，校稿过程中发现的少量小问题远比不上这个关键词意思的把握上。 分面可以让我们按照某种给定的条件，对数据进行分组，然后分别画图。 在统计变换一节中，提到如果按切工分组作回归线，显然图会很乱，有了分面功能，我们可以分别作图。 ggplot(small, aes(x=carat, y=price))+geom_point(aes(colour=cut))+scale_y_log10() +facet_wrap(~cut)+stat_smooth()    3.10 主题（Theme） 通过ggplot画图之后，我们可能还需要对图进行定制，像title, xlab, ylab这些高频需要用到的，自不用说，ggplot2提供了ggtitle(), xlab()和ylab()来实现。 比如： p &lt;- ggplot(small)+geom_boxplot(aes(x=cut, y=price,fill=color)) p + ggtitle(&quot;Price vs Cut&quot;)+xlab(&quot;Cut&quot;)+ylab(&quot;Price&quot;)  但是这个远远满足不了需求，我们需要改变字体，字体大小，坐标轴，背景等各种元素，这需要通过theme()函数来完成。 ggplot2提供一些已经写好的主题，比如theme_grey()为默认主题，我经常用的theme_bw()为白色背景的主题，还有theme_classic()主题，和R的基础画图函数较像。 别外ggthemes包提供了一些主题可供使用，包括： theme_economist theme_economist_white theme_wsj       theme_excel theme_few       theme_foundation theme_igray     theme_solarized theme_stata     theme_tufte require(ggthemes) ## Loading required package: ggthemes p + theme_wsj()  在2013年发表的文章Putative cobalt- and nickel-binding proteins and motifs in Streptococcus pneumoniae中的图3就是使用theme_stata来画的。 至于如何改变这些元素，我觉得我之前画囧字的博文可以做为例子： f &lt;- function(x) 1/(x^2-1) x &lt;- seq(-3,3, by=0.001) y &lt;- f(x) d &lt;- data.frame(x=x,y=y)   p &lt;- ggplot() p &lt;- p+geom_rect(fill = &quot;white&quot;,color=&quot;black&quot;,size=3, aes(NULL, NULL,xmin=-3, xmax=3, ymin=-3,ymax=3, alpha=0.1))   p &lt;- p + geom_line(data=d, aes(x,y), size=3)+ylim(-3,3)  theme_null &lt;- function() {     theme_bw() %+replace%     theme(axis.text.x=element_blank(),     axis.text.y=element_blank(),     legend.position=&quot;none&quot;,     panel.grid.minor=element_blank(),     panel.grid.major=element_blank(),     panel.background=element_blank(),     axis.ticks=element_blank(),     panel.border=element_blank()) }   p+theme_null()+xlab(&quot;&quot;)+ylab(&quot;&quot;)  详细的说明，可以参考?theme的帮助文档。   3.11 二维密度图 在这个文档里，为了作图方便，我们使用diamonds数据集的一个子集，如果使用全集，数据量太大，画出来散点就糊了，这种情况可以使用二维密度力来呈现。 ggplot(diamonds, aes(carat, price))+ stat_density2d(aes(fill = ..level..), geom=&quot;polygon&quot;)+ scale_fill_continuous(high=&#39;darkred&#39;,low=&#39;darkgreen&#39;)    3.12 ggplot2实战 蝴蝶图，详见《Modern Applied Statistics with S-PLUS》第一章。 theta &lt;- seq(0,24*pi, len=2000) radius &lt;- exp(cos(theta)) - 2*cos(4*theta) + sin(theta/12)^5 dd &lt;- data.frame(x=radius*sin(theta), y=radius*cos(theta)) ggplot(dd, aes(x, y))+geom_path()+theme_null()+xlab(&quot;&quot;)+ylab(&quot;&quot;)  这个图，我想展示的是对细节的修改上，在画囧字的时候，把画布上的元素都给清楚了，我把它定义为theme_null主题，在这里，直接应用，我们可以形成自己的画图风格，并写出自己的主题函数固定下来。 最后以生物界中常用的柱状图+误差图为实例，展示ggplot2非常灵活的图层。以我2011年发表的文章Phosphoproteome profile of human lung cancer cell line A549中的westernblot数据为例。这个实例展示了图层，标尺，主题，注释和各种细节微调多种元素。 Normal &lt;- c(0.83, 0.79, 0.99, 0.69) Cancer &lt;- c(0.56, 0.56, 0.64, 0.52) m &lt;- c(mean(Normal), mean(Cancer)) s &lt;- c(sd(Normal), sd(Cancer)) d &lt;- data.frame(V=c(&quot;Normal&quot;, &quot;Cancer&quot;), mean=m, sd=s) d$V &lt;- factor(d$V, levels=c(&quot;Normal&quot;, &quot;Cancer&quot;))  p &lt;- ggplot(d, aes(V, mean, fill=V, width=.5)) p &lt;- p+geom_errorbar(aes(ymin=mean, ymax=mean+sd, width=.2),                       position=position_dodge(width=.8)) p &lt;- p + geom_bar(stat=&quot;identity&quot;, position=position_dodge(width=.8), colour=&quot;black&quot;) p &lt;- p + scale_fill_manual(values=c(&quot;grey80&quot;, &quot;white&quot;)) p &lt;- p + theme_bw() +theme(legend.position=&quot;none&quot;) + xlab(&quot;&quot;) + ylab(&quot;&quot;) p &lt;- p + theme(axis.text.x = element_text(face=&quot;bold&quot;, size=12),                 axis.text.y = element_text(face=&quot;bold&quot;, size=12)) p &lt;- p+scale_y_continuous(expand=c(0,0), limits=c(0, 1.2), breaks=seq(0, 1.2, by=.2)) p &lt;- p+geom_segment(aes(x=1, y=.98, xend=1, yend=1.1)) p &lt;- p+geom_segment(aes(x=2, y=.65, xend=2, yend=1.1)) p &lt;- p+geom_segment(aes(x=1, y=1.1, xend=2, yend=1.1)) p &lt;- p + annotate(&quot;text&quot;, x=1.5, y=1.08, label=&quot;*&quot;) print(p)  更多实例，可以参考我使用ggplot2实现&lt;25 Recipes for Getting Started with R&gt;一书中的所有图片. 2013-05-18   "],
["section-4.html", "4 统计检验基础 4.1 概率分布 4.2 统计检验与p值 4.3 Confidence Intervals", " 4 统计检验基础  4.1 概率分布  4.1.1 均值与方差 所有离散分布的均值和方差都可以用以下公式计算： \\[\\mu=\\sum[xP(x)]\\] \\[\\sigma^2=\\sum[(x-\\mu)^2P(x)]\\]   4.1.2 二项式分布 二项式分布非常简单，形象点说白球和黑球有放回抽样。 \\[ P(X=r) = {n \\choose r} p^r (1-p)^{n-r}\\] 二项式分布的均值： \\[ \\mu = np \\] 方差： \\[ \\sigma^2 = np(1-p)\\] n越大，二项式分布越接近于正态分布： bnd &lt;- function(n, p=0.5) {     x &lt;- seq(0, n)     prob &lt;- dbinom(x, n, p)     df &lt;- data.frame(x=x, prob=prob, n=n)     return(df) } require(plyr) ## Loading required package: plyr bn.df &lt;- mdply(data.frame(n=seq(10,30, 5)), bnd) ggplot(bn.df, aes(x, prob, group=n, color=factor(n)))+geom_point(shape=1)+geom_line()    4.1.3 Poisson分布 Poisson分布是二项式分布的极限形式,用于描述单位时间或空间里随机事件发生的次数，参数 \\(\\mu\\) 表示单位时间或空间里某事件平均发生的次数。 \\[P(X=x) = \\frac{e^{-\\lambda}\\lambda^x}{x!}\\] Poisson分布的均值和方差相等： \\(\\mu = \\sigma^2 = \\lambda\\) . Poisson分布是右偏分布，均值越大时，其对称性越好： pd &lt;- function(x=100, lambda) {     x &lt;- seq(0, x)     prob &lt;- dpois(x, lambda)     df &lt;- data.frame(x=x, prob=prob, lambda=lambda)     return(df) } p.df &lt;- mdply(data.frame(lambda=seq(10,60, 10)), pd) ggplot(p.df, aes(x, prob, group=lambda, color=factor(lambda)))+geom_point(shape=1)+geom_line()+facet_wrap(~lambda)+theme(legend.position=&quot;none&quot;)    4.1.4 正态分布 正态分布是边续型分布，我们关心的不是某个值的概率，而是概率密度函数下的面积，它的概率密度函数是： \\[ f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-(x-\\mu)^2/2\\sigma^2}\\] 所有的正态分布，都可以标准化为均值为0，标准误为1的标准正态分布： \\[ f(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2}\\] 其中 \\(z=\\frac{x-\\mu}{\\sigma}\\) .   4.1.5 t分布 如果总体是正态分布，那么样本均值的分布也呈正态，William S. Gosset发现了t分布，它在参数估计和统计检验上，比正态分布要好，在大样本上做的和正态分布一样好，而在小样本上，比正态分布好很多。 require(plyr) x &lt;- seq(-3, 3, length=100) pn &lt;- dnorm(x) pn.df &lt;- data.frame(x=x, prob=pn)  get.pt &lt;- function(x, df) {     prob &lt;- dt(x, df=df)     dd &lt;- data.frame(x=x, prob=prob, df=df)     return(dd) } para &lt;- data.frame(x=rep(x, 4), df= rep(c(3, 5, 15, 30), each=length(x))) pt.df &lt;- mdply(para, get.pt) require(ggplot2) ggplot(pt.df, aes(x, prob))+geom_line(aes(group=df, color=factor(df)))+geom_line(data=pn.df)  上图画了自由度为3，5，15和30的t分布，以及正态分布，随着自由度的增加，t分布越来越接近于正态分布，很多时候，我们只有小样本，那么可以说t分布是正态分布的保守版本.从图中可以观察到，t分布中心比较平，而尾巴比较”胖“，特别是自由度较小的时候，t分布的尾巴会比正态分布长，那么同样一个值，t分布计算出来的p-value会比正态分布大一点。 dnorm(2) ## [1] 0.05399097 dt(2, df=5) ## [1] 0.06509031   4.1.6 卡方分布 k个独立的标准正态分布变量的平方和服从自由度为k的 \\(\\chi^2\\) 分布，卡方分布常用于假设检验和置信区间的计算。 卡方分布被用于计算拟合优度，于观察到的分布和假设成立的分布之间；估算总体标准偏差(population standard deviation)和样本标准偏差(sample standard deviation)的区间。 pc &lt;- function(x=50, df) {     x &lt;- seq(0, x)     prob &lt;- dchisq(x, df)     pc.df &lt;- data.frame(x=x, prob=prob, df=df)     return(pc.df) } pc.df &lt;- mdply(data.frame(df=c(3,6,9,18)), pc) ggplot(pc.df, aes(x, prob, group=df, color=factor(df)))+geom_point(shape=1)+geom_line()+ggtitle(&quot;Chi-Square Distribution&quot;)  ### F分布 F分布也是基于自由度，所以也是一个分布家族，它的自由度有两个，分别对应于分子项和分母项，因为F值是方差的比值，所以只有正值，函数峰在1左右（1代表两个方差是相等的），非对称分布，呈现右偏。 pff &lt;- function(x=seq(0,6,0.1), df1, df2) {     y &lt;- df(x, df1, df2)     pf.df &lt;- data.frame(x=x, prob=y, df1=df1, df2=df2)     return(pf.df) }  pf.df &lt;- mdply(data.frame(df1=c(3,4,10), df2=c(10, 15, 29)), pff) ggplot(pf.df, aes(x, prob, group=interaction(df1,df2), color=interaction(df1, df2)))+geom_line()+geom_vline(xintercept=1, linetype=&quot;dashed&quot;) + theme(legend.position=c(.8, .8))  如果F值小于1，使用左尾巴进行p值计算，大于1则右尾巴进行p值计算，很多人抛弃使用左尾巴进行p值计算，只需要把分子分母倒过来就行，当然自由度也要跟着倒过来。 虽然使用单侧来计算，但p值的计算通常是two-tailed，因为F值是两个方差的比值，不管两个均值的差是正的还是负的，方差都是正的。从这个角度上看，计算出来的单侧p值要乘以2，要把正和负的均值差都考虑在内，这是双侧检验。 假如F值为4.5，自由度为10和15，p值应该这样算： 2*(1-pf(4.5, 10, 15)) ## [1] 0.00921466    4.2 统计检验与p值  4.2.1 零假设 一切都是偶然的，真理只存在于概率分布中                     ---《女士品茶》 统计分析无法证明一个假设的真实性，它所提供的是假设存在的可能性，可能性（概率）是我们选择支持或反对假设的证据。 要证明一个假设是比较困难的，比如想证明“暨大图书馆前面的广场上有鸟”，一只鸟都没有看到，是不够的，它有可能就藏在某个树丛中。相反要推翻一个假设，却是相对要容易些，要推翻“暨大图书馆前面的广场上没有鸟”，我们没找到鸟，当然没有足够的证据去推翻它，但是但凡发现有一只鸟，这个假设就可以被推翻。 所以在统计学上，假设都是以“无罪假设”出现，称之为零假设(null hypothesis)，假设是关于总体参数的论断，而零假设总是“无罪“论断，如没有差别，没有效果，没有变化，没有关系等，而备择假设总是”有罪“论断。 我们以零假设为基础，计算概率(p值)，做出统计推断。如果p值小于阈值，则拒绝零假设，接受备择假设。 有时候会有学生找我说，帮忙算个p值，我就会问，你的零假设是什么？这句话等同于问“你想干嘛？”，神奇的是，有些时候有些人就真的不知道自己想干嘛！   4.2.2 P值  4.2.2.1 概率密度函数 假设不知道概率分布，我们收集了很多数据，那么这个数据可以用直方图表示出来，如果是离散型的数据，我们可以计算每个取值的概率值，比如二项式分布： dbinom(3, 10, 0.2) ## [1] 0.2013266 choose(10,3)*0.2^3*0.8^7 ## [1] 0.2013266 p &lt;- dbinom(0:10, 10, .2) print(p) ##  [1] 0.1073741824 0.2684354560 0.3019898880 0.2013265920 0.0880803840 ##  [6] 0.0264241152 0.0055050240 0.0007864320 0.0000737280 0.0000040960 ## [11] 0.0000001024 sum(p) ## [1] 1 如果数据是连续型的，就比较不好办了，它有无数个可能的取值，每一个精确的取值，概率都是0。我们只能计算某个区间的概率。我们把连续型的数据用直方图表示，把它当成离散型，就可以计算每个区间(bin)的概率，我们可以这样做处理，不断细化直方图的区间，当bin趋于0的时候，就可以得到每一取值的概率。事实上当bin趋于0的时候，直方图就变得光滑了，形状如果density curve。而density的计算，正是bin的概率除以bin的宽度，所以对于连续型函数，概率分布用的就是概率密度函数。   4.2.2.2 p value 我们有了概率密度函数，假设我们的数据来自标准正态分布，而观察值是3，那就可以计算出p值了，p值计算的是随机观察到如此极限的值（2）以及更极限的值（&gt;2）的概率。 x=seq(-4, 4, length=500) d &lt;- data.frame(x=x, prob=dnorm(x)) ggplot(d, aes(x, prob, fill=x&gt;=2))+geom_area()+scale_fill_manual(values=c(&quot;FALSE&quot;=&quot;steelblue&quot;, &quot;TRUE&quot;=&quot;red&quot;))+theme(legend.position=&quot;none&quot;)  也就是图中红色部分的面积。通常以一个阈值（比如0.05）来区分是否具有统计学显著性，这是以前查表时代的东西，现在统计学软件的大量使用，直接报p值比较好。 pnorm(2, lower.tail=F) ## [1] 0.02275013 上面红色部分算出来的p值是0.02275，我们可以拒绝总体是标准正态分布的零假设，这样的观察值来自于标准正态分布的概率是0.02275，也就是有0.02275的可能性零假设是对的，这就是我们拒绝零假设所犯错的可能性，称之为I类错误。 另外如果p值比较大于阈值，没有足够的证据拒绝零假设，有可能比如样本量太少等各种原因造成的，无法拒绝错误的零假设称之为II类错误。   I类和II类错误  广为接受的 \\(\\alpha\\) 水平是0.05，而power水平是0.80。     4.3 Confidence Intervals  4.3.1 均值置信区间 使用样本均值估计总体均值很容易，但要给出一个置信区间，相对就要困难些，因为这取决于数据的分布，如果数据是正态分布的，或者是大样本，那也比较容易，如果 \\(\\sigma\\) 已知，使用正态分布，如果未知，使用t分布。 但是如果数据不呈正态，样本量又比较小（n &lt; 30)的情况下，那只能使用非参或bootstrapping的方法了，相对就复杂点。  4.3.1.1 中心极限定理 get_sample_mean &lt;- function(size, population, n=1000) {     sapply(1:n, function(i) {         x &lt;- sample(population, size)         mean(x)     }) } pp &lt;- c(rnorm(1000, 0, 1), rnorm(100, 100, 1)) lsm &lt;- get_sample_mean(50, pp) ssm &lt;- get_sample_mean(10, pp) y &lt;- get_sample_mean(5, rnorm(1000)) par(mfrow=c(1,3)) hist(ssm, breaks=30, main=&quot;sample means distribution\\nfrom non-Normal distribution\\n(sample size = 10)&quot;) hist(lsm, breaks=30, main=&quot;sample means distribution\\nfrom non-Normal distribution\\n(sample size = 50)&quot;) hist(y, breaks=30, main=&quot;sample means distribution\\nfrom Normal distribution\\n(sample size = 5)&quot;)  只要样本量足够大，均值的分布都将呈现正态分布，不管样本来源于什么样的数据分布中，这就是中心极限定理，这也是为什么正态分布令人着迷之处。 从上面的图中，我们可以看到，如果样本量足够大，即使总体不呈正态，样本均值分布符合中心极限定理，也呈正态分布；如果样本量不够大，则数据来源必须是正态分布，均值的分布才是呈正态分布的。   4.3.1.2 样本均值标准误 从总体抽出一个样本，可以计算样本均值，我们通常使用样本均值来估计总体均值，反复抽样，每次得到的样本均值肯定是稍有不同的，我们需要用方差来量化这种不确定性，如果总体本身是正态分布，或者样本足够大，那么均值呈正态分布，均值为总体均值，而标准误差是我们需要估计的，有了标准误差，就可以通过正态分布，给出置信区间。 对于标准误差，有一词叫标准差（standard deviation, SD）还有一词叫标准误（standard error, SE），很容易混淆，我们拿到一个样本，对样本观察值离散程度的量化是SD: \\(sd(x)\\) ，而我们可以从很多个样本中得到很多个均值，这些均值的离散度用SE来量化， 也就是 \\(SE=sd(\\bar{x})\\) 。Campbell和Machin在他们的著作《Medical Statistics: a Commonsense Approach》中有一句话，值得铭记于心： If the purpose is Descriptive use standard Deviation; if the purpose is Estimation use standard Error. 对于均值的SE，通常称之为SEM（standard error of the means）。SEM显然和总体方差 \\(\\sigma^2\\) 以及样本量n有关，总体离散度高，SEM也会大一些；另一方面，样本量越大，对总体的估计就越好，SEM就会越小，这个还是很好理解的。 x1 &lt;- get_sample_mean(20, rnorm(1000, sd=1)) x2 &lt;- get_sample_mean(20, rnorm(1000, sd=3)) x3 &lt;- get_sample_mean(50, rnorm(1000, sd=3)) par(mfrow=c(1,2)) hist(x2, breaks=30, col=&quot;green&quot;, main=&quot;sample means distribution&quot;, xlab=&quot;Means&quot;) hist(x1, breaks=30, col=&quot;red&quot;, add=TRUE) legend(legend=c(expression(paste(sigma==1)), expression(paste(sigma==3))), fill=c(&quot;red&quot;, &quot;green&quot;), &quot;topright&quot;) hist(x2, breaks=30, col=&quot;green&quot;, main=&quot;sample means distribution&quot;, xlab=&quot;Means&quot;) hist(x3, breaks=30, col=&quot;red&quot;, add=TRUE) legend(legend=c(&quot;n=10&quot;, &quot;n=50&quot;), fill=c(&quot;green&quot;, &quot;red&quot;), &quot;topright&quot;)  从上面的仿真抽样可以看到，SEM和 \\(\\sigma\\) 呈正相关，\\(\\sigma\\) 越大，SEM越大；而和sample size呈负相关，sample size越大，SEM越小。 实际上： \\[\\sigma_{\\bar{x}}^2=\\frac{\\sigma^2}{n}\\] 也就是： \\[\\sigma_{\\bar{x}}=\\sqrt{\\frac{\\sigma^2}{n}} = \\frac{\\sigma}{\\sqrt{n}}\\] 通常情况下，总体 \\(\\sigma\\) 是未知的，使用样本sd来估计，那么SEM为： \\[s_{\\bar{x}} = \\frac{s}{\\sqrt{n}}\\] 我们可以通过仿真抽样进行验证： ## generate 1000 sample with sample size 100 a = sapply(rep(100, 1000), rnorm) a.mean = colMeans(a) ## estimate SEM by simulation sd(a.mean) ## [1] 0.1003199 ## estimate SEM by sigma/sqrt(n), sigma = 1 1/sqrt(100) ## [1] 0.1 ## estimate SEM by sample 1 sd(a[, 1])/sqrt(100) ## [1] 0.09954846   4.3.1.3 使用正态分布估计置信区间 样本均值呈正态分布的情况下，可以使用正态分布和t分布来估计置信区间，用那种方法，取决于总体参数 \\(\\sigma\\) 是否已知。 知道 \\(\\sigma\\) 的情况下，使用正态分布。 \\[ (\\bar{x}-z_{\\alpha/2}SEM) &lt; \\mu &lt; (\\bar{x}+z_{\\alpha/2}SEM)\\] 也就是： \\[ (\\bar{x}-z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}) &lt; \\mu &lt; (\\bar{x}+z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}})\\] 这里从 \\(\\mu=1, \\sigma=3\\) 的总体中随机抽一个样本，计算95%的置信区间： set.seed(123) n &lt;- 100 sigma &lt;- 3 alpha &lt;- 0.05 x &lt;- rnorm(n, mean=1, sd=sigma) m &lt;- mean(x) sem &lt;- sigma/sqrt(n) #margin of error me &lt;- qnorm(1-alpha/2)*sem msg &lt;- paste((1-alpha)*100,&quot;% &quot;, &quot;Confidence Interval: [&quot;, round(mean(x)-me, 3), &quot;, &quot;, round(mean(x)+me,3), &quot;]&quot;, sep=&quot;&quot;) print(msg) ## [1] &quot;95% Confidence Interval: [0.683, 1.859]&quot;   4.3.1.4 使用t分布估计置信区间 在很多情况下 \\(\\sigma\\) 是未知的，那么就得使用t分布来进行置信区间估计， 形式和正态分布是一样的，只不过用t值替代了z值，因为t分布的尾巴比正态长，所以置信区间的宽度会大一些，特别是在自由度小的时候。 \\[ (\\bar{x}-t_{\\alpha/2}SEM) &lt; \\mu &lt; (\\bar{x}+t_{\\alpha/2}SEM)\\] 在未知 \\(\\sigma\\) 的情况下，使用sd来估计 \\(\\sigma\\) ，最终公式为： \\[ (\\bar{x}-t_{\\alpha/2}\\frac{s}{\\sqrt{n}}) &lt; \\mu &lt; (\\bar{x}+t_{\\alpha/2}\\frac{s}{\\sqrt{n}})\\] t.test()函数进行t检验时，会计算出置信区间: t.test(rnorm(100)) ##  ##  One Sample t-test ##  ## data:  rnorm(100) ## t = -1.1122, df = 99, p-value = 0.2688 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ##  -0.29941792  0.08432432 ## sample estimates: ##  mean of x  ## -0.1075468   4.3.1.5 误差幅度 置信区间的估计，就是 \\(\\bar{x}\\) \\(\\pm\\) \\(z_{\\alpha/2}\\sigma_{\\bar{x}}\\) ，\\(z_{\\alpha/2}\\sigma_{\\bar{x}}\\)这个量被称之为误差幅度（margin of error）: \\[E=z_{\\alpha/2}\\sigma_{\\bar{x}}\\] 很容易可以推导出： \\[ n=(\\frac{z_{\\alpha/2}\\sigma_{\\bar{x}}}{E})^2\\] E是n的函数，样本量n越大，E就越小；反过来，n也是E的函数，如果我们想要把误差幅度限制在一个比较小的范围，那么就要加大样本量，上面的公式就给出了对于样本量的估计。 estimateSampleSize &lt;- function(E, sigma, alpha=0.05) {     ## E is the margin of error     n &lt;- ((qnorm(alpha/2)*sigma)/E)^2     ceiling(n) }  estimateSampleSize(3, 10) ## [1] 43    4.3.2 比例的置信区间 很多数据会以比例的形式出现，比如对某事件进行调查，会报道出支持和反对的比例；生物苦逼娃可能会计算不同形态的细胞比例。 比例的置信区间，由以下公式给出： \\[ (\\hat{p} - E) &lt; p &lt; (\\hat{p} + E)\\] 其中E是误差幅度， \\(\\hat{p}\\) 是样本计算出来的比例，而p是总体比例。 E通过以下公式计算： \\[ E = z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\] 这种情况符合二项式分布，而n又比较大，所以可以用正态分布来估计，那么这公式就很好理解了，和均值的误差幅度估计是一样的，都是 \\(z_{\\alpha/2}SE\\) ，而SE是 \\(\\sigma/\\sqrt{n}\\) 。我们知道二项式分布 \\(\\sigma^2 = np(1-p)\\) ，这里p是未知的，使用样本统计量 \\(\\hat{p}\\) 来估计，就可以推出E的计算公式。 假设数了1000个细胞，700个是梭形的，我们想估计梭形细胞所点比例的95%置信区间，就可以通过以上公式计算： \\[ E = 1.96\\sqrt{\\frac{(0.7)(0.3)}{1000}} = 1.96\\sqrt{\\frac{0.21}{1000}} = 1.96\\sqrt{0.00021} = 0.0284\\] (0.7 - 0.0284) &lt; p &lt; (0.7 + 0.0284) 95%置信区间为 [0.6716, 0.7284]   4.3.3 方差和标准差的置信区间 通常拿样本SD来估计总体SD，要给出包含总体SD的置信区间，需要用到卡方分布。 假设我们从正态分布的总体中抽出样本量为n的样本，总体的方差为 \\(\\sigma^2\\) ，那么样本方差为 \\(s^2\\) 的函数符合卡方分布： \\[ \\chi^2 = \\frac{(n-1)s^2}{\\sigma^2}\\] 卡方是不对称分布，所以置信区间也是不对称的，需要分别找出左侧和右侧的临界值（critical value）。 假设n是100，那么自由度是99，我们要计算95%的置信区间，需要分别计算左侧0.025和右侧0.025的临界值： qchisq(0.025, 99, lower.tail=TRUE) ## [1] 73.36108 qchisq(0.025, 99, lower.tail=FALSE) ## [1] 128.422 这两个数称之为卡方左右值 \\(\\chi_{L}^2\\) 和 \\(\\chi_{R}^2\\) ，那么标准差的置信区间为： \\[\\sqrt{\\frac{(n-1)s^2}{\\chi_{R}^2}} &lt; \\sigma &lt; \\sqrt{\\frac{(n-1)s^2}{\\chi_{L}^2}}\\]   4.3.4 均值差的置信区间 我们记两组数据为 \\(x_1, x_2\\) ，其均值分别为 \\(\\bar{x_1}, \\bar{x_2}\\) ，总体均值为 \\(\\mu_1, \\mu_2\\) ，总体均值差为 \\(\\mu_d = \\mu_1 - \\mu_2\\) 。 ### 成对数据均值差的置信区间 假设想检验某药对某病是否有用，我们观察一群病人，用药前和用药后的指标，这样的数据就是成对数据，用药前和用药后的数据是相关的。 假设 \\(d_i\\) 是第i个病人用药前后的差值： \\[ d_i = x_{1i} -x_{2i} \\] 那么统计量 \\(\\bar{x_1}, \\bar{x_2} = \\bar{d}\\) 。 成对数据均值差 \\(\\bar{x_1}, \\bar{x_2}\\) 的分布就变成了差值均值 \\(\\bar{d}\\) 的分布，问题就变成了均值置信区间的估计，可以直接套用t分布的估计公式： \\[ (\\bar{d}-t_{\\alpha/2}SEM) &lt; \\mu_d &lt; (\\bar{x}+t_{\\alpha/2}SEM)\\] 问题在于估计标准误SEM，样本均值的标准误前面已经讲过 \\(SEM=\\frac{s}{\\sqrt{n}}\\) ，这里的观察值为 \\(d_i\\) ，于是： \\[ SE_\\bar{d} = \\frac{s_d}{\\sqrt{n}}\\] 所以成对数据均值差 \\(\\mu_d\\) 的置信区间为： \\[ (\\bar{d}-t_{\\alpha/2}\\frac{s_d}{\\sqrt{n}}) &lt; \\mu_d &lt; (\\bar{d}+t_{\\alpha/2}\\frac{s_d}{\\sqrt{n}})\\]  4.3.4.1 两样本均值差的置信区间 如果两组数据是独立的，那么情况要复杂一些，但原理还是一样，只要是正态分布或者样本量足够大，那么均值差的分布就呈现正态分布，依然是使用t分布来做参数估计： \\[ ((\\bar{x_1} - \\bar{x_2})-t_{\\alpha/2}SEDM) &lt; \\mu_1 - \\mu_2 &lt; ((\\bar{x_1} - \\bar{x_2})+t_{\\alpha/2}SEDM)\\] 其中SEDM，代表standard error of difference of means。  4.3.4.1.1 方差相同 方差相同的情况下，对SEDM的估计就比较容易，把两个样本的方差合并(pool)，合并方差(pooled variance)： \\[s_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2}\\] 继而SEDM由以下公式计算： \\[SEDM = \\sqrt{\\frac{s_p^2}{n_1}\\frac{s_p^2}{n_2}}\\] 自由度为 \\(n_1+n_2-2\\) .   4.3.4.1.2 方差不同 方差不同使用Welch方法，不对方差进行合并。 \\[SEDM = \\sqrt{\\frac{s_1^2}{n_1}\\frac{s_2^2}{n_2}}\\] 自由度通过Welch-Satterhwaite公式计算： \\[ df = \\frac{(s_1^2/n_1+s_2^2/n2)^2}{\\frac{(s_1^2/n_1)^2}{n_1-1}+\\frac{(s_2^2/n_2)^2}{n_2-1}}\\]     4.3.5 相关性置信区间 Pearson correlation coefficient的置信区间，参考相关性一节。    "],
["t.html", "5 T检验 5.1 单样本T检验 5.2 Z检验 5.3 两样本T检验", " 5 T检验  5.1 单样本T检验  5.1.1 前提条件 使用T检验，前提条件必须是满足正态分布，如果样本量足够多的时候，根据中心极限定理，数据分布不呈正态也是没问题的。   5.1.2 数据标准化 在满足前提条件的基础上，我们假定均值的分布是呈正态的。 简单点说，我们的任务是看样本均值偏离总体均值有多远，越远则p值越小，这是绝对正相关的。那么直接计算 \\(| \\bar{x} - \\mu |\\) 行不行？显然可以，但是it depends。 m1 &lt;- get_sample_mean(5, rnorm(1000)) m2 &lt;- get_sample_mean(5, rnorm(1000, sd=3))  hist(m2, col=&quot;green&quot;, main=&quot;Histogram of sample means&quot;, xlab=&quot;Means&quot;, breaks=30) hist(m1, col=&quot;red&quot;, add=T, breaks=30) legend(legend=c(expression(paste(sigma==1)), expression(paste(sigma==3))), fill=c(&quot;red&quot;, &quot;green&quot;), &quot;topright&quot;)  我们知道正态分布由两个参数决定，均值和方差，所以 \\(| \\bar{x} - \\mu |\\) 的大小，不能直接对应到p值，从上图绿色中抽到一个\\(| \\bar{x} - \\mu | = 3\\) 的样本，概率并不低，但从红色分布中得到这样一个样本，几乎是不可能的。 拿 \\(| \\bar{x} - \\mu |\\) 来度量的话，你必须提供 \\(sd(\\bar{x})\\) ，\\(| \\bar{x} - \\mu |\\) 不能直接比较。 如果\\(sd(\\bar{x})\\) 相同的话，\\(| \\bar{x} - \\mu |\\) 就是很好的度量方法，它是可比较的，值越大，p值越小。那么我们需要做的，就是把数据压缩到同一个scale上，既然均值分布呈正态分布，而所有的正态分布都可以scale到标准正态分布上，显然我们可以用相同的方法来处理，把数据统一scale到 \\(SEM = 1\\) ，于是差值 \\(| scale(\\bar{x}) - \\mu |\\) 就完全可以拿来比较，年纪大点的人，上学时候p值是通过查表获得的，查表用的值就是 \\(| scale(\\bar{x}) - \\mu |\\) 。    5.2 Z检验 在讲T检验前，先回顾一下Z检验，标准正态分布称之为Z分布，根据Z分布来做统计检验，便是Z检验。 我们把一个正态分布scale到标准正态分布上，通过: \\[ \\frac{x-\\mu}{sd}\\] 在这里检验的统计量是均值 \\(\\bar{x}\\) ，均值的sd，也就是SEM， \\(SEM=\\frac{\\sigma}{\\sqrt{n}}\\) ，通过 \\(\\frac{sd(x)}{\\sqrt{n}}\\) 估计，具体参考置信区间一节， 所以我们使用以下公式，把均值分布scale到z分布上： \\[z= \\frac{\\bar{x}-\\mu}{sd(x)/\\sqrt{n}}\\] 那么通过z值在z分布上的位置，就可以算出p值，进行z检验。  5.2.1 t检验  In theory, there is no difference between theory and practice. But, in practice, there is.                                                          -- Jan L.A. van de Snepscheut  虽然可以直接利用正态分布来计算p值，然而理想状态很丰满，现实却很骨感，如果样本量n较大时，不单t分布逼近正态分布， \\(\\chi^2_{(n)}\\) ,Poisson( \\(lambda\\) ),和二项分布B(n,p)都逼近正态，然而我们没有那么大的样本量，所以在不理想的现实中，有各种各样的检验来应对各种情景和现实条件。 我们要通过样本参数来估计总体参数，特别是对于小样本来说，非常不靠谱。可以说正态是理想状态，而T分布是其现实版本。T分布中心比正态分布平，尾巴比正态分布长，特别是在自由度较小的情况下，所以T检验计算出来的p值比Z检验要保守些。 相应的，t值的计算和z值的计算是一样的。 \\[t=\\frac{\\bar{x}-\\mu}{sd(x)/\\sqrt{n}}\\] 使用R进行单样本的T检验，是一件非常容易的事情。 x &lt;- sample(1:100, 10) print(x) ##  [1] 83 73 63 46 56 92 32 10 12  8 t.test(x, mu=20) ##  ##  One Sample t-test ##  ## data:  x ## t = 2.8014, df = 9, p-value = 0.02066 ## alternative hypothesis: true mean is not equal to 20 ## 95 percent confidence interval: ##  25.29383 69.70617 ## sample estimates: ## mean of x  ##      47.5    5.3 两样本T检验 和单样本T检验一样，前提条件是数据来自于正态分布，或者是样本量足够大，在这种情况下，两样本均值差的分布是呈现正态的，便可以应用T分布来计算p值。 在理解了正态分布、T分布以及p值的计算之后，我们应该很清楚，T检验就是通过计算t值来计算显著性： \\[t = \\frac{X-\\mu}{SE}\\] 这里的X是我们的统计量，如单样本的 \\(\\bar{x}\\) ,而这里两样本，则为 \\(\\bar{x_1}-\\bar{x_2}\\) ，而SE是统计量X的标准误，单样本T检验我们使用SEM来表示均值的标准误(standard error of the means)，相应的两样本T检验，我们使用SEDM来代表均值差的标准误（standard error of difference of means）。 两样本t检验的通用形式为： \\[ t = \\frac{(x_1-x_2)-(\\mu_1-\\mu_2)}{SEDM}\\] 由于零假设是均值没有差别，所以 \\(\\mu_1-\\mu_2\\) 会被假定为0. T检验的问题就在于如何计算SE，在这里要做两样本T检验，问题就成了如何计算SEDM。 在置信区间一节里，已经介绍了如何估计SEDM。  5.3.1 成对T检验 成对数据的 \\(SEDM = = \\frac{s_d}{\\sqrt{n}}\\) 。 成对T检验本质上和单样本T检验是一样的，通过计算 \\(d_i = x_{1i} -x_{2i}\\) ，对d值进行单样本t检验。 data(iris) attach(iris) t.test(Sepal.Length, Petal.Length, paired=TRUE) ##  ##  Paired t-test ##  ## data:  Sepal.Length and Petal.Length ## t = 22.813, df = 149, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ##  1.904708 2.265959 ## sample estimates: ## mean of the differences  ##                2.085333 d = Sepal.Length - Petal.Length t.test(d) ##  ##  One Sample t-test ##  ## data:  d ## t = 22.813, df = 149, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ##  1.904708 2.265959 ## sample estimates: ## mean of x  ##  2.085333   5.3.2 方差相同的两样本T检验 这种情况下 \\(SEDM = = \\sqrt{\\frac{s_p^2}{n_1}+\\frac{s_p^2}{n_2}}\\) , 自由度是 \\(n_1+n_2-2\\) 。 其中 \\(s_p^2\\) 是合并方差： \\(s_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2}\\) 很多教科书所说的两独立样本T检验就是指这种情况。记得以前老师讲过说T检验必须满足两个条件，一是正态，二是方差齐性，说的就是这个。 当然T检验只需满足一个条件，那就是正态，方差不齐的情况下面会讲到。 方差是否相同，可以计算两个样本的方差比值，方差相同，则比值应该在1左右，而方差不同，则比值会偏离1，这个比值称为F ratio，可以使用F检验来计算方差齐性的显著性。在方差齐性的前提条件下，我们使用上面的公式进行T检验。   5.3.3 方差不同的两样本T检验 如果F检验把方差齐性的零假设给reject了，那么就得用Welch t检验。 \\[SEDM = \\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}\\] 自由度通过Welch-Satterhwaite公式计算： \\[ df = \\frac{(s_1^2/n_1+s_2^2/n2)^2}{\\frac{(s_1^2/n_1)^2}{n_1-1}+\\frac{(s_2^2/n_2)^2}{n_2-1}}\\]   5.3.4 使用R进行T检验 R的stats包提供了t.test函数，可以用于各种T检验，如果只提供一组数据，则进行单样本T检验，如果提供两组数据，则进行两样本T检验，主要的参数，无非是paired，TRUE则进行成对t检验，var.equal指定方差是否齐性，TRUE则进行经典方法，FALSE则进行Welch T检验。 使用实例可参考RT-PCR一节。   5.3.5 结论 成对数据我们可以把成对的信息扔了，混在一起做两独立样本T检验，方差齐了，我们也可以把它当做不齐，用Welch方法。所以如果稀里糊涂的情况下，就用Welch T检验。 如果是成对数据，当然还是成对T检验好，比如病人在使用某药物前后的指标，如果不用成对，则病人之间的variance也混进去，方差估计会大一些，T检验的power也会减弱。 Welch T检验的自由度会比方差齐性的经典方法要小，根据T分布，自由度越小，中心越平，而尾巴越长，也就是说，观察到同样一个t值，自由度小的分布计算出来的p值会更大，换句话说，自由度越小，T检验就越保守。 这也是t.test函数默认使用Welch T test的原因，Welch T test较为保守，如果方差齐性，用经典方法可以检验出更小的差别。    "],
["section-6.html", "6 方差分析 6.1 One-Way ANOVA 6.2 TukeyHSD - Tukey honestly significant difference 6.3 Two-Way ANOVA 6.4 Advanced ANOVA", " 6 方差分析  6.1 One-Way ANOVA T检验只能比较两样本均值，而方差分析（analysis of variance, ANOVA）能够同时比较多个均值，ANOVA通过分析方差来计算均值是否和总体有显著性差异，ANOVA把方差分为处理效应（treatment effect，真实差异)和误差（error，抽样误差或个体差异）两个来源，两个方差的比值服从F分布，因为方差本身就是个体和均值差平方和，所以对方差组成的分析能够反映均值的差异。 “处理”项是均值间的差异，而“误差”项是组内的差异，习惯性地称之为组间(between)差异和组内(within)差异。 把所有分组混在一起，当成一个样本，总的方差为： \\[SS_{tot} = \\sum(x-\\bar{x})^2\\] 将其分为组间方差和组内方差： \\[SS_{tot} = SS_b+SS_w\\] 其中组间方差为： \\[SS_b=\\sum_{j=1}^k n_j(\\bar{x_j}-\\bar{\\bar{x}})^2\\] 组间自由度是k-1，k是分组的个数，通常将组间方差除以组间自由度，得到组间均方(between groups mean square)，代表组间均值的处理效应。 组内方差为： \\[SS_w=\\sum_{j=1}^k\\sum_{i=1}^{n_j}(x_{ij}-\\bar{x_j})^2\\] 组内自由度是N-k，N是把所有分组当成一个样本的样本量，通常将组内方差除以组内自由度，得到组内均方(within groups mean square)，代表组内误差。 方差分析所使用的均方，也就是上面公式所计算的方差除以相应的自由度。  6.1.1 单向方差分析 data(mpg) mpg$cyl=factor(mpg$cyl) head(mpg) ##   manufacturer model displ year cyl      trans drv cty hwy fl   class ## 1         audi    a4   1.8 1999   4   auto(l5)   f  18  29  p compact ## 2         audi    a4   1.8 1999   4 manual(m5)   f  21  29  p compact ## 3         audi    a4   2.0 2008   4 manual(m6)   f  20  31  p compact ## 4         audi    a4   2.0 2008   4   auto(av)   f  21  30  p compact ## 5         audi    a4   2.8 1999   6   auto(l5)   f  16  26  p compact ## 6         audi    a4   2.8 1999   6 manual(m5)   f  18  26  p compact boxplot(displ~cyl, data=mpg)  如果我们根据不同汽缸数（cylinders, cyl变量）对汽车进行分组，分析发动机排量（engine displacement, displ变量）均值是否不同，从图上可以看到，汽缸数越多，排量越大。 res=aov(displ~cyl, data=mpg) summary(res) ##              Df Sum Sq Mean Sq F value Pr(&gt;F)     ## cyl           3  339.7  113.22   528.9 &lt;2e-16 *** ## Residuals   230   49.2    0.21                    ## --- ## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 one way ANOVA的零假设是所有均值都相等，备择假设是至少有一个均值是有差别的。 这里计算出来的p值非常小，意味着reject零假设，接受备择假设。 one way anova给出的是总体信息，并没有对均值间两两进行统计的信息，并不清楚分组间谁和谁有显著性差异，想要知道这个信息，需要再进行后续分析。 我们可以先看一下均值： model.tables(res, type=&quot;means&quot;) ## Tables of means ## Grand mean ##           ## 3.471795  ##  ##  cyl  ##          4   5      6      8 ##      2.146 2.5  3.409  5.133 ## rep 81.000 4.0 79.000 70.000    6.2 TukeyHSD - Tukey honestly significant difference 对one way anova进行后续分析有很多方法，方差分析由R. A. Fisher提出，他设计出Fisher LSD (least significant difference)标准，现在很多统计学者认为这个方法太自由，现在普遍使用的是Tukey HSD检验，比Fisher LSD要保守得多。 TukeyHSD(res) ##   Tukey multiple comparisons of means ##     95% family-wise confidence level ##  ## Fit: aov(formula = displ ~ cyl, data = mpg) ##  ## $cyl ##          diff        lwr       upr     p adj ## 5-4 0.3543210 -0.2589862 0.9676282 0.4421149 ## 6-4 1.2631817  1.0738406 1.4525229 0.0000000 ## 8-4 2.9871781  2.7917721 3.1825842 0.0000000 ## 6-5 0.9088608  0.2951884 1.5225332 0.0009354 ## 8-5 2.6328571  2.0172865 3.2484278 0.0000000 ## 8-6 1.7239964  1.5274470 1.9205458 0.0000000 它会计算不同组两两之间的差异、置信区间和较正后的p值。 TukeyHSD并不仅限于单向方差分析，它可以对各种方差分析进行后续的分析。  6.2.1 两两T检验 如果只是多组实验，只需要两两比较均值的话，可以对分组两两进行T检验，当然必须进行p值较正，因为随着统计检验次数的增加，犯I类错误的概率会不断增大。方差分析对于复杂的实验设计是很有优势的，比如多因素，重复测量数据。 stats包里提供了pairwise.t.test()函数，可以进行两两T检验，还能对p值进行较正，使用起来很方便： with(mpg, pairwise.t.test(displ, cyl, p.adjust.method=&quot;bonferroni&quot;)) ##  ##  Pairwise comparisons using t tests with pooled SD  ##  ## data:  displ and cyl  ##  ##   4       5       6       ## 5 0.81757 -       -       ## 6 &lt; 2e-16 0.00098 -       ## 8 &lt; 2e-16 &lt; 2e-16 &lt; 2e-16 ##  ## P value adjustment method: bonferroni   6.2.2 如何理解方差分析 单向方差分析是最简单的，因为只有一个因素，双向方差分析考虑两个因素，理解了这两种ANOVA分析，那么对于多因素N-WAY ANOVA自然也不在话下。而一些高级的ANOVA可以针对复杂的实验设计，虽然复杂，但其基础思想是高度一致的。 很多人可能有疑问，为什么分析方差可以来检验均值？方差本身就是离均差的平方和，但这样的解释似乎不够。 我们来看一下T检验，T检验的形式是 \\(t=\\frac{\\bar{x}-\\mu}{SE}\\) ,我在T检验一节里，讲这相当于把数据归一化到标准正态分布上，也可以这么讲，把差值放到标准误SE的尺度上，我们要看的是差值有多大，但要把它放到SE的尺度上来看，SE代表的是统计量 \\(\\bar{x}\\) 的离散度，衡量的是不确定性。从这个角度上看，我们可以把分子当成是信号，而分母当成是噪声，于是t值计算的就是信噪比。 再来看方差分析，有多组均值，零假设是这些均值没有差别。只有一两组的时候，使用T检验，信号是差值，但现在有多组，我们把差值加和起来的话，差值可正可负，正负加和会抵消，所以很自然的，把差值进行平方运算，当然要使用样本量进行加权，因为样本量越大的数据，越可靠，权重要大一些，于是信号（分子）是 \\(SS_b=\\sum_{j=1}^k n_j(\\bar{x_j}-\\bar{\\bar{x}})^2\\) ，这就是组间方差，当然最终的信号是均方，需要除以自由度，以消除分组数目的影响。 分母是对数据不确定性的度量，也就是噪声，通过组内数据方差来估计。 对于j分组，样本方差为： \\(s_j = \\sum_{i=1}^{n_j} (x_{ij} - \\bar{x_j})^2\\) 均值误差为： \\(SE_j = \\sqrt{\\frac{s_j^2}{n_j}}\\) 那么我们可以使用各个分组的均值误差对总体均值误差进行估计： \\[SE_{tot}^2 = \\frac{\\sum_{j=1}^k (n_j-1)SE_j^2}{\\sum_{j=1}^k n_j - k} \\\\ = \\frac{\\sum_{j=1}^k (n_j-1) \\frac{s_j^2}{n_j}}{\\sum_{j=1}^k n_j - k} \\\\ \\simeq \\frac{\\sum_{j=1}^k s_j^2}{\\sum n_j - k}\\] 从上面的公式可以发现分子 \\(\\sum_{j=1}^k s_j^2\\) 是组内方差，而分母 \\(\\sum n_j - k\\) 是自由度N-k，总体误差的估计就是方差分析中的误差均方。 所以方差分析和T检验是高度一致的，分子是信号，T检验中是差值，方差分析中为了消除差值正负的影响，用差值平方和，也就是方差。 分母是噪声，也就是度量不确定性的误差，T检验中使用标准误SE，而方差分析中使用方差，也就是标准误SE的平方。 分子度量差别到底有多大，然后把它放在不确定性的尺度(分母)上看，这就是T检验和方差分析的原理。    6.3 Two-Way ANOVA ANOVA可以分析两个或多个因子的复合效应，ANOVA的设计越复杂，对结果的解析同样会变得很复杂。   双向ANOVA  双向ANOVA分析是单向ANOVA的扩展，分析两个因素，我们称之为A和B，假如A有r个水平，B有c个水平，则总共有rxc个分组，每个分组的数据数目要一致。  6.3.1 前提条件 使用Two-Way ANOVA，需要满足以下假设： + 总体须是正态分布或接近于正态分布 + 样本必须是独立样本 + 方差齐性 + 分组样本量一样   6.3.2 零假设 双向ANOVA能够同时检验3个零假设： + 单独考虑A因素，总体均值间没有差别。这相当于对A因子进行单向ANOVA + 单独考虑B因素，总体均值间没有差别。这相当于对B因子进行单向ANOVA + A和B两个因素，没有相互作用。这相当于使用二联表进行独立性分析   6.3.3 方差计算 data &lt;- read.table(&quot;data/gender_dose.tsv&quot;, header=TRUE) data ##    Observation Gender Dosage Alertness ## 1            1      m      a         8 ## 2            2      m      a        12 ## 3            3      m      a        13 ## 4            4      m      a        12 ## 5            5      m      b         6 ## 6            6      m      b         7 ## 7            7      m      b        23 ## 8            8      m      b        14 ## 9            9      f      a        15 ## 10          10      f      a        12 ## 11          11      f      a        22 ## 12          12      f      a        14 ## 13          13      f      b        15 ## 14          14      f      b        12 ## 15          15      f      b        18 ## 16          16      f      b        22 我们来分析上面这份数据，看Gender和Dosage两个因素对Alerness水平的影响。Gender和Dosage都是2个水平，这是最简单的2x2设计。 进行双向方差分析，需要计算6个方差。  6.3.3.1 总方差 不对数据进行分组，计算出来的方差，为总方差，自由度为样本量减1. grand.mean &lt;- with(data, mean(Alertness)) N &lt;- with(data, length(Alertness)) N ## [1] 16 SS.tot &lt;- with(data, sum((Alertness-grand.mean)^2)) SS.tot ## [1] 392.9375   6.3.3.2 单元格方差 两个因素A和B，分别有r和c个水平，则组成rxc的二联表，把数据分成rxc个组，和单向ANOVA分析一样，计算组间方差： \\[ SS_{tot} = \\sum(x-\\bar{\\bar{x}})^2\\] require(plyr) grp &lt;- ddply(data, .(Gender, Dosage), function(x) data.frame(n=length(x$Alertness), m=mean(x$Alertness))) grp ##   Gender Dosage n     m ## 1      f      a 4 15.75 ## 2      f      b 4 16.75 ## 3      m      a 4 11.25 ## 4      m      b 4 12.50 SS.cells &lt;- with(grp, sum(n*(m-grand.mean)^2)) SS.cells ## [1] 81.6875   6.3.3.3 因素A的方差 只考虑因素A，对于这个数据来说，是性别因素，只使用性别因素对数据进行分组，按单向ANOVA分析一样，计算组间方差： Am &lt;- with(data, tapply(Alertness, Gender, mean)) An &lt;- with(data, tapply(Alertness, Gender, length)) SS.gender &lt;- sum(An * (Am - grand.mean)^2) SS.gender ## [1] 76.5625 自由度是因素A的水平减1，既r-1，这里r=2，所以df=1。   6.3.3.4 因素B的方差 只考虑因素B，对于这个数据来说，是药剂用量因素，只使用dosage因素对数据进行分组，按单向ANOVA分析一样，计算组间方差： Bm &lt;- with(data, tapply(Alertness, Dosage, mean)) Bn &lt;- with(data, tapply(Alertness, Dosage, length)) SS.dosage &lt;- sum(Bn * (Bm - grand.mean)^2) SS.dosage ## [1] 5.0625 自由度是因素B的水平减1，既c-1，这里c=2，所以df=1。   6.3.3.5 因素A和B相互作用的方差 单元格方差由A因素和B因素组成，可以拆分为因素A的方差、因素B的方差和AB互作的方差，即： \\[ SS_{cells} = SS_A + SS_B + SS_{AB}\\] 所以A和B互作的方差： \\[ SS_{AB} = SS_{cells} - SS_A - SS_B\\] SS.gender.dosage &lt;- SS.cells - SS.gender - SS.dosage SS.gender.dosage ## [1] 0.0625 自由度是A的自由度乘以B的自由度，即：(r-1)(c-1)。   6.3.3.6 误差方差 最后是组内方差，它度量误差，方差分析和线性回归是高度一致的，在R的aov函数里，会把误差方差写成残差方差，这其实来自于线性回归。 dw &lt;- ddply(data, .(Gender, Dosage), function(x) x$Alertness - mean(x$Alertness)) dw ##   Gender Dosage    V1    V2    V3    V4 ## 1      f      a -0.75 -3.75  6.25 -1.75 ## 2      f      b -1.75 -4.75  1.25  5.25 ## 3      m      a -3.25  0.75  1.75  0.75 ## 4      m      b -6.50 -5.50 10.50  1.50 sum(dw[,-c(1,2)]^2) ## [1] 311.25 上面按照A和B两个因素，分成rxc组，按单向ANOVA分析方法，计算组内方差。 实际上，误差方差等于总方差减去单元格方差，也就是总方差中不能由因素A和B解释的，就是误差方差： SS.err = SS.tot - SS.cells SS.err ## [1] 311.25 自由度是总自由度减去A，B和AB互作的自由度，即(N-1) - (r-1) - (c-1) - (r-1)(c-1).    6.3.4 p值计算 计算了这6个方差之后，就要以计算F值，进而计算p值。计算F值需要用均方，即方差除以自由度。分子相当于信号，而分母相当于噪声。信噪比足够高，则这个因素有作用。 pf(SS.gender/1 / (SS.err/12), df1=1, df2=12, lower.tail=F) ## [1] 0.1114507 pf(SS.dosage/1 / (SS.err/12), df1=1, df2=12, lower.tail=F) ## [1] 0.6664956 pf(SS.gender.dosage/1 / (SS.err/12), df1=1, df2=12, lower.tail=F) ## [1] 0.9616567 上面分别看了Gender, Dosage, Gender和Dosage相互作用的p值，分别对应于前面提到的三个零假设。   6.3.5 ANOVA using R stats包中的aov()函数，可以进行双向方差分析，我们并不需要手工计算这么多的统计量。 summary(with(data, aov(Alertness~Gender*Dosage))) ##               Df Sum Sq Mean Sq F value Pr(&gt;F) ## Gender         1  76.56   76.56   2.952  0.111 ## Dosage         1   5.06    5.06   0.195  0.666 ## Gender:Dosage  1   0.06    0.06   0.002  0.962 ## Residuals     12 311.25   25.94 虽然这里相互作用没有显著性，但做为演示，依然可以画一下相互作用图，使用stats包提供的interaction.plot，如果有显著性，可以从图中看出效应。 with(data, interaction.plot(Gender, Dosage, Alertness))  这个图其实就是以Gender为X轴，按Dosage进行分组，以分组的Alertness均值为Y轴，进行画图，自己使用ggplot2来画也是非常简单的，还可以给均值加上errorbar或confidence interval，都是很容易的事情。 require(ggplot2) grp &lt;- ddply(data, .(Gender, Dosage), function(x) data.frame(m=mean(x$Alertness))) ggplot(grp, aes(Gender, m, group=Dosage, shape=Dosage, color=Dosage, linetype=Dosage))+geom_point()+geom_line()  对于分组数据均值是否有差异，也可以画boxplot或者是使用均值和置信区间来画图。    6.4 Advanced ANOVA  6.4.1 Repeated-Measures ANOVA ANOVA分析要求测量值是独立的，但是很多情况下，并不独立，比如对一个病人用药后不同时间段进行测量，同一个病人的测量值显然是相关的。 这可以看成是成对T检验的扩展，可以应用于同一对象多个测量值的情况，所以称之为重复测量方差分析。 假如我们有以下数据： data &lt;- read.table(&quot;data/aov2.tsv&quot;, header=T) data ##    Observation Subject Task Valence Recall ## 1            1     Jim Free     Neg      8 ## 2            2     Jim Free     Neu      9 ## 3            3     Jim Free     Pos      5 ## 4            4     Jim Cued     Neg      7 ## 5            5     Jim Cued     Neu      9 ## 6            6     Jim Cued     Pos     10 ## 7            7  Victor Free     Neg     12 ## 8            8  Victor Free     Neu     13 ## 9            9  Victor Free     Pos     14 ## 10          10  Victor Cued     Neg     16 ## 11          11  Victor Cued     Neu     13 ## 12          12  Victor Cued     Pos     14 ## 13          13    Faye Free     Neg     13 ## 14          14    Faye Free     Neu     13 ## 15          15    Faye Free     Pos     12 ## 16          16    Faye Cued     Neg     15 ## 17          17    Faye Cued     Neu     16 ## 18          18    Faye Cued     Pos     14 ## 19          19     Ron Free     Neg     12 ## 20          20     Ron Free     Neu     14 ## 21          21     Ron Free     Pos     15 ## 22          22     Ron Cued     Neg     17 ## 23          23     Ron Cued     Neu     18 ## 24          24     Ron Cued     Pos     20 ## 25          25   Jason Free     Neg      6 ## 26          26   Jason Free     Neu      7 ## 27          27   Jason Free     Pos      9 ## 28          28   Jason Cued     Neg      4 ## 29          29   Jason Cued     Neu      9 ## 30          30   Jason Cued     Pos     10 按照普通的方差分析 summary(aov(Recall~Task*Valence, data=data)) ##              Df Sum Sq Mean Sq F value Pr(&gt;F) ## Task          1   30.0   30.00   1.749  0.198 ## Valence       2    9.8    4.90   0.286  0.754 ## Task:Valence  2    1.4    0.70   0.041  0.960 ## Residuals    24  411.6   17.15 如果这样来做，忽略了同一受试对象(subject)各个观察值之间的相关性这一信息，好比把成对T检验的成对信息给扔了一样。 方差分析按照因素进行分组，误差均方即为组间均方，因为数据是独立，但上面的数据并不独立。这里如果只按照因素进行分组，而不考虑受试对象的信息，则把受试对象内部的方差也给归入误差方差，像上面这种重复测量的数据，受试对象内部的方差应该从误差方差中分离出来，归入真实方差。   重复测量ANOVA  重复测量方差分析和其它方差分析的差别在于对方差的划分不同。 summary(aov(Recall~Task*Valence+Error(Subject/(Task*Valence)), data=data)) ##  ## Error: Subject ##           Df Sum Sq Mean Sq F value Pr(&gt;F) ## Residuals  4  349.1   87.28                ##  ## Error: Subject:Task ##           Df Sum Sq Mean Sq F value Pr(&gt;F)   ## Task       1  30.00  30.000   7.347 0.0535 . ## Residuals  4  16.33   4.083                  ## --- ## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ##  ## Error: Subject:Valence ##           Df Sum Sq Mean Sq F value Pr(&gt;F) ## Valence    2   9.80   4.900   1.459  0.288 ## Residuals  8  26.87   3.358                ##  ## Error: Subject:Task:Valence ##              Df Sum Sq Mean Sq F value Pr(&gt;F) ## Task:Valence  2   1.40   0.700   0.291  0.755 ## Residuals     8  19.27   2.408 把受试对象内部的方差从误差方差中分离出去，误差项变小了，F值变大，p值变小，检验的power大了很多。   6.4.2 Mixed-Factorial ANOVA md &lt;- read.table(&quot;data/aov.mixed.tsv&quot;, header=T) head(md) ##   Obs Subject Gender Dosage Task Valence Recall ## 1   1       A      M      A    F     Neg      8 ## 2   2       A      M      A    F     Neu      9 ## 3   3       A      M      A    F     Pos      5 ## 4   4       A      M      A    C     Neg      7 ## 5   5       A      M      A    C     Neu      9 ## 6   6       A      M      A    C     Pos     10 tail(md) ##     Obs Subject Gender Dosage Task Valence Recall ## 103 103       R      F      C    F     Neg     19 ## 104 104       R      F      C    F     Neu     17 ## 105 105       R      F      C    F     Pos     19 ## 106 106       R      F      C    C     Neg     22 ## 107 107       R      F      C    C     Neu     21 ## 108 108       R      F      C    C     Pos     20 上面这个数据，有18个对象，9个男性9个女性，每个对象被使用了三种可能剂量的药，然后被测试使用两种记忆类型（cued和free call），能否回忆起三种类型的词(positive, negative和neutral)，这里有2个对象间变量：性别和剂量，2个对象内剂量：Task（2个水平）和Valence（3个水平）。 下面的命令，对 对象内因子、对象内误差项、对象间因子 进行方差分析。 summary(aov(Recall~(Task*Valence*Gender*Dosage)+Error(Subject/(Task*Valence))+(Gender*Dosage),data=md)) ##  ## Error: Subject ##               Df Sum Sq Mean Sq F value Pr(&gt;F)   ## Gender         1  542.3   542.3   5.685 0.0345 * ## Dosage         2  694.9   347.5   3.643 0.0580 . ## Gender:Dosage  2   70.8    35.4   0.371 0.6976   ## Residuals     12 1144.6    95.4                  ## --- ## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ##  ## Error: Subject:Task ##                    Df Sum Sq Mean Sq F value   Pr(&gt;F)     ## Task                1  96.33   96.33  39.862 3.87e-05 *** ## Task:Gender         1   1.33    1.33   0.552    0.472     ## Task:Dosage         2   8.17    4.08   1.690    0.226     ## Task:Gender:Dosage  2   3.17    1.58   0.655    0.537     ## Residuals          12  29.00    2.42                      ## --- ## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ##  ## Error: Subject:Valence ##                       Df Sum Sq Mean Sq F value Pr(&gt;F)   ## Valence                2  14.69   7.343   2.998 0.0688 . ## Valence:Gender         2   3.91   1.954   0.798 0.4619   ## Valence:Dosage         4  20.26   5.065   2.068 0.1166   ## Valence:Gender:Dosage  4   1.04   0.259   0.106 0.9793   ## Residuals             24  58.78   2.449                  ## --- ## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ##  ## Error: Subject:Task:Valence ##                            Df Sum Sq Mean Sq F value Pr(&gt;F) ## Task:Valence                2   5.39  2.6944   1.320  0.286 ## Task:Valence:Gender         2   2.17  1.0833   0.531  0.595 ## Task:Valence:Dosage         4   2.78  0.6944   0.340  0.848 ## Task:Valence:Gender:Dosage  4   2.67  0.6667   0.327  0.857 ## Residuals                  24  49.00  2.0417 多元回归的F值和ANOVA的F值是一样的，事实上两者的底层是general linear model，R在计算ANOVA时使用的是多元回归的特例。    "],
["correlation.html", "7 Correlation 7.1 协方差 (Covariance) 7.2 相关性 (Correlation) 7.3 相关性统计检验 7.4 置信区间", " 7 Correlation 度量两个变量的相关性，对于数量型数据，通常使用Pearson correlation coefficient： \\[ r = \\frac{\\sum_{i=1}^n (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2)}\\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2)}}\\] 相关系数r接近于1，表明强正相关，接近于-1，表明强负相关，接近于0，表明没有相关性。  7.1 协方差 (Covariance) 要理解相关系数，首先要知道什么是协方差，它被定义为： \\[\\sigma_{xy} = \\frac{\\sum(x-\\mu_x)(y-\\mu_y)}{N}\\] 通常情况下，总体是未知的，我们手头上只有样本，相应的样本的计算公式为： 研究两个变量的关系，可以使用相关系数来度量相关性的强度，也可以用简单回归分析把相关性用直线方程表示出来。 \\[s_{xy} = \\frac{\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})}{n-1}\\] x和y的协方差取值可以是正，负和0，如果协方差是正的，x上升和y上升相关；如果协方差是负的，x上升和y下降相关。   7.2 相关性 (Correlation) 协方差的值受x和y度量单位的影响，为了得到一个无标度(scaleless)的统计量，将它除以x和y的标准误： \\[ r_{xy} = \\frac{s_{xy}}{s_x s_y} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2)}\\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2)}}\\] 于是我们得到的，就是Pearson Correlation Coefficient. 协方差和相关系数很容易计算，R提供了cov()和cor()函数分别用于计算协方差和相关系数，输入参数可以是向量，也可以是矩阵，如果是矩阵，将对每个column两两计算： data(iris) head(iris) ##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1          5.1         3.5          1.4         0.2  setosa ## 2          4.9         3.0          1.4         0.2  setosa ## 3          4.7         3.2          1.3         0.2  setosa ## 4          4.6         3.1          1.5         0.2  setosa ## 5          5.0         3.6          1.4         0.2  setosa ## 6          5.4         3.9          1.7         0.4  setosa with(iris, cov(Sepal.Length, Petal.Length)) ## [1] 1.274315 r &lt;- with(iris, cor(Sepal.Length, Petal.Length)) print(r) ## [1] 0.8717538 plot(iris[,-5], col=rainbow(3)[as.numeric(iris[,5])])  cov(iris[,-5]) ##              Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length    0.6856935  -0.0424340    1.2743154   0.5162707 ## Sepal.Width    -0.0424340   0.1899794   -0.3296564  -0.1216394 ## Petal.Length    1.2743154  -0.3296564    3.1162779   1.2956094 ## Petal.Width     0.5162707  -0.1216394    1.2956094   0.5810063 cor(iris[,-5]) ##              Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411 ## Sepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259 ## Petal.Length    0.8717538  -0.4284401    1.0000000   0.9628654 ## Petal.Width     0.8179411  -0.3661259    0.9628654   1.0000000 采集越多的数据，肯定计算出来的相关性越接近了真实值，换一批数据，计算出来的结果肯定也会有一些细微差别。 我们要问两个问题： + r值和0的差别多大，才可以肯定相关性是真实存在的？ + 从样本计算的r值，是对总体r值的估计，能否计算出置信区间？   7.3 相关性统计检验 零假设： \\[ H_{0}: pr = 0\\] \\[H_{a}: pr\\; is\\; non-zero.\\] 其中pr代表population r。 这里需要用到Fisher’s z transformation： \\[z_r = \\frac{1}{2} \\log_e(\\frac{1+r}{1-r})\\] 对相关系数r进行转换，转换后的值，将服从均值为 \\(\\frac{1}{2} \\log_e(\\frac{1+pr}{1-pr})\\) ，标准误为 \\(\\frac{1}{\\sqrt{n-3}}\\) 的正态分布。 这里 $ H_{0}: pr = 0$ 所以均值 \\(\\frac{1}{2} \\log_e(\\frac{1+pr}{1-pr})=0\\) ，那么就可以使用正态分布来计算p value。 Ztrans &lt;- function(r) 1/2 * log((1+r)/(1-r)) zr &lt;- Ztrans(r) n &lt;- nrow(iris) zr.sd &lt;- 1/sqrt(n-3) ## p-value: pnorm(r, mean=0, sd=zr.sd, lower.tail=FALSE) ## [1] 2.064483e-26   7.4 置信区间 既然转换后的 \\(z_r\\) 值服从正态分布，很空间可以获得 \\(z_r\\) 的置信区间，但是我们的目的是相关系数r的置信区间，这需要通过把 \\(z_r\\) 值反转回r值。 revZ &lt;- function(z) (exp(2*z)-1)/(exp(2*z)+1)  lwzr &lt;- zr - 1.96 * zr.sd upzr &lt;- zr + 1.96 * zr.sd lwr &lt;- revZ(lwzr) upr &lt;- revZ(upzr) msg &lt;- paste(&quot;95% confidence interval [&quot;, round(lwr,3), &quot;, &quot;, round(upr,3), &quot;]&quot;, sep=&quot;&quot;) print(msg) ## [1] &quot;95% confidence interval [0.827, 0.906]&quot;   "],
["section-8.html", "8 线性回归 8.1 Simple Linear Regression 8.2 Multiple Linear Regression", " 8 线性回归  8.1 Simple Linear Regression  Many shall be restored that now are fallen;   Many shall be fallen that now are in honor.  相关性是简单线性回归它爹，回归的概念来自于Francis Galton，他发现高个子男人生出来的儿子会比自己矮，而矮个子男人生出来的儿子会比自己高，并称这种现象为回归平庸(regression toward mediocrity)，现在被称之为回归均值(regression to the mean)，Galton有个学生叫Karl Pearson，他给出了相关性和回归的数学公式，这也是相关系数被命名为Pearson相关系数的原因，而相关系数的符号r则取自于回归(regression)一词。 简单线性回归名副其实，非常简单，在假定X和Y是线性关系，使用单变量X来预测Y。 \\[ Y \\approx \\beta_0 + \\beta_1 X \\] 假设我们要用花萼长度来预测花瓣长度， require(ggplot2) data(iris) attach(iris) ## The following objects are masked from iris (pos = 3): ##  ##     Petal.Length, Petal.Width, Sepal.Length, Sepal.Width, Species p &lt;- ggplot(iris, aes(Sepal.Length, Petal.Length))+         geom_point(shape=1, color=&quot;red&quot;) print(p)  我们需要拟合： \\[ Petal \\approx \\beta_0 + \\beta_1 Sepal \\] \\(\\beta_0\\) 和 \\(\\beta_1\\) 代表线性模型的截距和斜率，这两个模型参数是未知的，需要通过训练数据估计 \\(\\hat{\\beta_0}\\) 和 \\(\\hat{\\beta_1}\\) 。 使得直线 \\[ \\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} x \\] 与数据集中的点最接近，有多种方法来来计算“接近度“，最常用的是最小二乘法(least squares)。 最小二乘法通过计算残差平方和（RSS, residual sum of squares） \\[ RSS = {e_1}^2 + {e_2}^2 + … + {e_n}^2 \\] 其中 \\(e_i = y_i - \\hat{y_i}\\) . 问题转换为找出 \\(\\hat{\\beta_0}\\) 和 \\(\\hat{\\beta_1}\\) 使得RSS的值最小。 通过计算，可以得到： \\[ \\hat{\\beta_1} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = r_{xy} (\\frac{s_y}{s_x})\\] \\[ \\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x} \\] b1 &lt;- sum((Petal.Length-mean(Petal.Length)) * (Sepal.Length-mean(Sepal.Length)))/sum((Sepal.Length-mean(Sepal.Length))^2) b0 &lt;- mean(Petal.Length) - b1 * mean(Sepal.Length) cat(&quot;Intersect:\\t&quot;, b0, &quot;\\n&quot;, &quot;Slope:\\t&quot;, b1, &quot;\\n&quot;) ## Intersect:    -7.101443  ##  Slope:   1.858433 stats包中的lm()函数，用于拟合线性模型。 model &lt;- lm(Petal.Length ~ Sepal.Length, data=iris) model ##  ## Call: ## lm(formula = Petal.Length ~ Sepal.Length, data = iris) ##  ## Coefficients: ##  (Intercept)  Sepal.Length   ##       -7.101         1.858  8.1.1 评估参数准确性 打个比方，我们使用样本均值 \\(\\hat{\\mu}\\) 来估计总体均值 \\(\\mu\\) ，对于任意一个样本的 \\(\\hat{\\mu}\\) 值，有可能会高估 \\(\\mu\\) ，也有可能会低估，需要量化到底 \\(\\hat{\\mu}\\) 偏离 \\(\\mu\\) 有多远，这个问题通过计算standard error of \\(\\hat{\\mu}\\) 来解决。 同样地，我们估计出来的参数 \\(\\hat{\\beta_0}\\) 和 \\(\\hat{\\beta_1}\\) 和真实的参数到底偏离多远，需要通过计算参数的standard error来估计。 \\[ SE(\\hat{\\beta_0})^2 = \\sigma^2 [\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n (x_i-\\bar{x})^2}] \\] \\[ SE(\\hat{\\beta_1})^2 = \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i-\\bar{x})^2} \\] 其中 \\(\\sigma^2 = Var(\\epsilon)\\) ，通常情况下是未知的，使用残差标准误 \\(RSE=\\sqrt{RSS/(n-2)}\\) 来估计。 p+geom_smooth(method=&quot;lm&quot;, se=TRUE, level=0.95)  上图中，阴影部分就是参数的95%置信区间。 有了标准误，还可以用统计检验来检测X和Y是否具有相关性。 在这里，可以使用t检验，计算t统计量： \\[ t = \\frac{\\hat{\\beta_1} - 0}{SE(\\hat{\\beta_1})} \\] 这些统计量，lm()函数都会计算。 summary(model) ##  ## Call: ## lm(formula = Petal.Length ~ Sepal.Length, data = iris) ##  ## Residuals: ##      Min       1Q   Median       3Q      Max  ## -2.47747 -0.59072 -0.00668  0.60484  2.49512  ##  ## Coefficients: ##              Estimate Std. Error t value Pr(&gt;|t|)     ## (Intercept)  -7.10144    0.50666  -14.02   &lt;2e-16 *** ## Sepal.Length  1.85843    0.08586   21.65   &lt;2e-16 *** ## --- ## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ##  ## Residual standard error: 0.8678 on 148 degrees of freedom ## Multiple R-squared:   0.76,  Adjusted R-squared:  0.7583  ## F-statistic: 468.6 on 1 and 148 DF,  p-value: &lt; 2.2e-16   8.1.2 评估模型准确性  8.1.2.1 残差标准误(RSE, residual standard error) iris$fitted &lt;- predict(model) p %+% iris +  aes(x=fitted, y=Petal.Length-fitted) + geom_linerange(aes(ymin = 0, ymax = Petal.Length - fitted), colour = &quot;purple&quot;) + geom_hline(aes(yintercept = 0)) + ggtitle(&quot;Residual Distribution&quot;)+ylab(&quot;Residual&quot;)  RSE是Y值和回归直线偏离值均值： \\[ RSE = \\sqrt{\\frac{RSS}{n-2}} = \\sqrt{\\frac{\\sum_{i=1}^n (y_i-\\hat{y_i})^2}{n-2}} \\] rse &lt;- with(iris, sqrt(sum((fitted - Petal.Length)^2)/(length(Petal.Length)-2))) print(rse) ## [1] 0.8678147 通过RSE，可以计算模型预测值和真实值平均水平偏离多少。偏离量大不大，可以用 \\(RSE/\\bar{y}\\) 来估计。 with(iris, rse/mean(Petal.Length)) ## [1] 0.2309246 RSE度量的是失拟（lack of fit），如果RSE很小，则 \\(\\hat{y_i}\\) 和 \\(y_i\\) 很接近，模型对数据的拟合非常好，如果RSE很大，则表明模型对数据的拟合很差。   8.1.2.2 \\(R^2\\) 统计量 RSE是绝对值，不够清晰，用 \\(RSE/\\bar{y}\\) 相对值会好一些。 \\(R^2\\) 提供另外一种度量方式: \\[ R^2 = \\frac{TSS - RSS}{TSS} = 1 - \\frac{RSS}{TSS}\\] 其中 \\(TSS=\\sum(y_i - \\bar{y})^2\\) ,TSS度量Y的方差，也就是拟合前总的方差；而RSS度量的是残差的方差，也就是拟合后无法解释的方差；TSS-RSS度量的是能够由拟合模型解释的方差；继而， \\(R^2\\) 统计量度量的是Y的方差能由X来解释的比例。 \\(R^2\\) 接近1，表明回归能解释Y的方差，回归模型拟合得好。而接近0的话，则表明无法解释Y的大部分方差，拟合模型很差，甚至可能是错的。 tss &lt;- with(iris, sum((Petal.Length - mean(Petal.Length))^2)) rss &lt;- with(iris, sum((fitted-Petal.Length)^2)) rr &lt;- 1 - rss/tss rr ## [1] 0.7599546 \\(R^2\\) 统计量度量的是X和Y的线性相关性，我们知道相关系数r定义为： \\[ Cor(X, Y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2)}\\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2)}}\\] 同样我们可以用相关系数r来评估模型，事实上 \\(R^2 = r^2\\) ，可以说 \\(R^2\\) 是 \\(r^2\\) 的通用形式，相关系数只能用于单变量，如果要用多变量做线性回归的话，就没法用，从这个角度来看，也可以说 \\(R^2\\) 是 \\(r^2\\) 的扩展形式。    8.1.3 方差分析 如前所述，Y的方差TSS，由两部分组成，残差方差(RSS)和回归方差(TSS-RSS)，继而我们可以进行方差分析，TSS的自由度是n-1, 回归方差是1(简单线性回归是单变量),残差方差的自由度是n-2 （TSS的df - 回归方差df），将方差除以自由度，得到平均方差。 如果不存在线性关系，那么回归平均方差和残差平均方差大致相等。可以使用F统计量来检验是否存在线性关系。 \\[ F = \\frac{regression\\; mean\\; square}{residual\\; mean\\; square} = \\frac{TSS-RSS}{RSS/(n-2)}\\] F统计量服从1和n-2自由度的F分析，继而可以计算出p值，当然可以直接扔给anova函数，进行统计计算。 n &lt;- nrow(iris) fstat &lt;- (tss-rss)/(rss/(n-2)) print(fstat) ## [1] 468.5502 pf(fstat, 1, n-2, lower.tail=F) ## [1] 1.038667e-47 anova(model) ## Analysis of Variance Table ##  ## Response: Petal.Length ##               Df Sum Sq Mean Sq F value    Pr(&gt;F)     ## Sepal.Length   1 352.87  352.87  468.55 &lt; 2.2e-16 *** ## Residuals    148 111.46    0.75                       ## --- ## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1   8.1.4 可视化辅助诊断模型 评估模型最重要的指标是残差，R提供了函数可视化残差，残差 vs 拟合值, 残差开方 vs 拟合值, 残差的QQ图，标准化残差 vs Leverage, leverage度量的是数据点对回归线的影响。 par(mfrow=c(2,2)) plot(model)     8.2 Multiple Linear Regression 在简单线性回归中，我们使用花萼长度来预测花瓣长度，在iris数据集里，还有花萼宽度、花瓣宽度的数据，如果我们想探索花萼宽度和花瓣宽度与花瓣长度的关系，可以分别做简单线性回归，这样子每一个简单线性回归，都忽略了其它两个因素的影响。事实上，一个现象常常与多个因素相联系，由多个变量组合共同来预测因变量，会更加有效。 多元线性回归模型和简单线性回归一样，每个变量需要一个斜率参数： \\[ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdot\\cdot\\cdot + \\beta_p X_p + \\epsilon \\] 假设这里有p个变量， \\(X_j\\) 代表第j个变量， \\(\\beta_j\\) 代表 \\(X_j\\) 每升高一个单位对Y的平均影响。 真实的参数是未知的，我们需要估计 \\(\\hat{\\beta}_0,\\hat{\\beta}_1,...,\\hat{\\beta}_p\\) 来估计回归参数 \\(\\beta_0,\\beta_1,...,\\beta_p\\) ，于是回归模型就变成： \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\cdot\\cdot\\cdot + \\hat{\\beta}_p x_p\\] 参数估计依然使用最小二乘法，找出参数 \\(\\hat{\\beta}_0,\\hat{\\beta}_1,...,\\hat{\\beta}_p\\) 使得RSS最小。 \\[ RSS= \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\\\ = \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_1 - \\hat{\\beta}_2 x_2 - \\cdot\\cdot\\cdot - \\hat{\\beta}_p x_p)^2 \\] 从数值计算上看，这是个优化问题，使用矩阵运算还是比较容易的，具体请戳这里。 在R里，依然可以使用lm函数来做多元线性回归： data(iris) lm.fit &lt;- lm(Petal.Length ~ Petal.Width+Sepal.Length+Sepal.Width, data=iris) lm.fit ##  ## Call: ## lm(formula = Petal.Length ~ Petal.Width + Sepal.Length + Sepal.Width,  ##     data = iris) ##  ## Coefficients: ##  (Intercept)   Petal.Width  Sepal.Length   Sepal.Width   ##      -0.2627        1.4468        0.7291       -0.6460  8.2.1 相关问题 进行多元线性回归，我们需要回答以下一些重要的问题： + X和Y是否存在关系? + 所有自变量都有助于解释Y吗？或者说是否只有一部分自变量对Y的预测是有用的？ + 模型对数据的拟合有多好？ + 预测的准确性有多好？  8.2.1.1 X和Y是否存在关系? 对于这个问题，可以使用在简单线性回归中提到的RSE和 \\(R^2\\) 来评估。 ## TSS tss &lt;- with(iris, sum((Petal.Length - mean(Petal.Length))^2)) ## if calculate prediction values manually, use the following command: ## b &lt;- lm.fit$coefficients ## iris$&quot;(Intercept)&quot; &lt;- 1 ## d &lt;- as.matrix(iris[, names(b)]) ## iris$fitted &lt;- d %*% as.matrix(b, ncol=1) ## RSS iris$fitted &lt;- predict(lm.fit) rss &lt;- with(iris, sum((fitted-Petal.Length)^2)) b &lt;- lm.fit$coefficients p &lt;- length(b) - 1 n &lt;- nrow(iris) df &lt;- n - p -1  ## RSE rse &lt;- sqrt( rss/df ) print(rse) ## [1] 0.3189554 ## R-squared rr &lt;- 1 - rss/tss print(rr) ## [1] 0.9680118 \\(R^2\\) 是很高的，证明X和Y确实是存在关系的。 另一方面，可以做统计检验，如果X和Y没有关系，那么参数 \\(\\beta_1 = \\beta_2 = \\cdot\\cdot\\cdot = \\beta_p = 0\\) ，我们可以检验零假设： \\[ H_{0}: \\beta_1 = \\beta_2 =... = \\beta_p = 0\\] \\[H_{a}: at\\; least\\; one\\; \\beta_j\\; is\\; non-zero.\\] 这个假设检验通过计算F统计量： \\[ F = \\frac{(TSS-RSS)/p}{RSS/(n-p-1)}\\] 其中 \\(TSS=\\sum(y_i - \\bar{y})^2\\) 而 \\(RSS=\\sum(y_i - \\hat{y}_i)^2\\) ，如果线性模型是正确的，那么： \\[E\\{RSS/(n-p-1)\\} = \\sigma^2\\] 如果 \\(H_0\\) 是对的，则 \\[E\\{(TSS - RSS)/p\\} = \\sigma^2\\] 因此如果 \\(H_0\\) 是对的，那么F统计量的值因为接近于1，如果 \\(H_a\\) 是对的，则 \\(E\\{(TSS - RSS)/p\\} &gt; \\sigma^2\\) ，F统计量要大于1。 ## F-statistic fstat &lt;- ((tss-rss)/p) / (rss/df) print(fstat) ## [1] 1472.726 F统计量服从F分布，可以根据F分布来计算p值，以决定是否reject \\(H_0\\) 。 pf(fstat, p, n-p-1, lower.tail=F) ## [1] 6.976868e-109 上面计算的这些统计量，lm函数都有计算。 summary(lm.fit) ##  ## Call: ## lm(formula = Petal.Length ~ Petal.Width + Sepal.Length + Sepal.Width,  ##     data = iris) ##  ## Residuals: ##      Min       1Q   Median       3Q      Max  ## -0.99333 -0.17656 -0.01004  0.18558  1.06909  ##  ## Coefficients: ##              Estimate Std. Error t value Pr(&gt;|t|)     ## (Intercept)  -0.26271    0.29741  -0.883    0.379     ## Petal.Width   1.44679    0.06761  21.399   &lt;2e-16 *** ## Sepal.Length  0.72914    0.05832  12.502   &lt;2e-16 *** ## Sepal.Width  -0.64601    0.06850  -9.431   &lt;2e-16 *** ## --- ## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ##  ## Residual standard error: 0.319 on 146 degrees of freedom ## Multiple R-squared:  0.968,  Adjusted R-squared:  0.9674  ## F-statistic:  1473 on 3 and 146 DF,  p-value: &lt; 2.2e-16 而且还对每个变量计算了p值，这些p值给出了每一个 \\(x_j\\) 和y是否相关的信息。   8.2.1.2 变量选择 通过上面输出中每个参数的p值，可以看出每个变量对y的贡献是不一样的，很多情况下，y只和其中某一部分 \\(x_j\\) 有关，这就涉及到变量选择问题。 针对这个问题有三种方法： + Forward selection：从空模型（只包含截距）开始，对p个变量分别做简单线性回归，对产生最小RSS的变量加入到模型中，继而做两变量拟合，把产生最小RSS的第二个变量，再加入到模型中，不断迭代直到停止条件产生。 + Backward selection：所有变量一起拟合，然后移除p值最大的变量，再对(p-1)个变量重新拟合，再移除最大p值的变量，不断迭代，直到停止条件出现。 + Mixed selection：这是Forward和Backward selection的组合，从空模型开始，按照Forward selection来做，变量一个个地加入，在这个过程中，某些变量的p值是有可能升高的，如果p值高于某个阈值，移除这个变量。不断地进行forward和backward步骤，直到模型中的所有变量p值都足够小，而模型外的变量，如果加入到模型中，会产生比较大的p值。 Backward selection不能应用于 p &gt; n的情况下，而Forward selection则可以，Forward selection是贪婪方法，开始加入的变量到了后面可能变成冗余，而Mixed selection可以弥补这一点。   8.2.1.3 模型拟合 模型对数据的拟合度有多好，可以使用之前计算过的RSE和 \\(R^2\\) 来评估。 简单线性回归给出的RSE公式，是针对简单线性回归的简化形式，其通过形式为： \\[ RSE = \\sqrt{\\frac{RSS}{n-p-1}} \\] 在简单线性回归中 \\(R^2\\) 是X和Y的相关系数的平方，在多元线性回归中，它等于Y和 \\(\\hat(Y)\\) 的相关系数的平方。事实上拟合后的模型，除了RSS最小之外， \\(R^2\\) 是最大的。 with(iris, cor(fitted, Petal.Length)^2) ## [1] 0.9680118 按相关系数计算的 \\(R^2\\) 和之前使用 \\(1 - \\frac{RSS}{TSS}\\) 计算的是一样的。   8.2.1.4 模型预测 参数预测本身是有误差的，即使我们知道真实的参数，也不可能完美地预测数据，因为模型中包含有随机误差 \\(\\epsilon\\) ，在预测的时候，最好使用置信区间，这样把uncertainty的信息也包括在内。 xx &lt;- predict(lm.fit, se.fit=TRUE, interval=&quot;confidence&quot;, level=0.95) xx &lt;- as.data.frame(xx$fit) xx$y &lt;- iris$Petal.Length head(xx) ##        fit      lwr      upr   y ## 1 1.484210 1.393923 1.574497 1.4 ## 2 1.661389 1.569631 1.753146 1.4 ## 3 1.386358 1.296915 1.475802 1.3 ## 4 1.378046 1.284284 1.471808 1.5 ## 5 1.346695 1.251095 1.442294 1.4 ## 6 1.733905 1.619998 1.847812 1.7 mean(with(xx, y&gt; lwr &amp; y &lt; upr)) ## [1] 0.3266667 这个模型的 \\(R^2\\) 是0.968，拟合得如此好的模型，预测起来，偏差还是有那么些，真实值落在预测的95%置信区间里，只占了35%不到。 yy &lt;- predict(lm.fit, se.fit=TRUE, interval=&quot;prediction&quot;, level=0.95)$fit ## Warning in predict.lm(lm.fit, se.fit = TRUE, interval = &quot;prediction&quot;, level = 0.95): predictions on current data refer to _future_ responses colnames(yy) &lt;- c(&quot;fitpred&quot;, &quot;lwrpred&quot;, &quot;uprpred&quot;) xx &lt;- cbind(xx, yy) head(xx) ##        fit      lwr      upr   y  fitpred   lwrpred  uprpred ## 1 1.484210 1.393923 1.574497 1.4 1.484210 0.8474110 2.121009 ## 2 1.661389 1.569631 1.753146 1.4 1.661389 1.0243794 2.298398 ## 3 1.386358 1.296915 1.475802 1.3 1.386358 0.7496783 2.023038 ## 4 1.378046 1.284284 1.471808 1.5 1.378046 0.7407447 2.015347 ## 5 1.346695 1.251095 1.442294 1.4 1.346695 0.7091209 1.984269 ## 6 1.733905 1.619998 1.847812 1.7 1.733905 1.0933304 2.374480 mean(with(xx, y&gt; lwrpred &amp; y &lt; uprpred)) ## [1] 0.9733333 显然用prediction方法，给出的预测值置信区间要靠谱得多。 ggplot(xx, aes(fit, y))+geom_point() + geom_line(aes(y=fit)) + geom_line(aes(y=lwr), color=&quot;red&quot;) +  geom_line(aes(y=upr), color=&quot;red&quot;) +  geom_line(aes(y=lwrpred), color=&quot;blue&quot;) +  geom_line(aes(y=uprpred), color=&quot;blue&quot;)     8.2.2 数值运算 参考以前的博文 \\[ B = (X^TX)^{-1}(X^TY)\\] X=iris[,c(&quot;Petal.Width&quot;, &quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;)] X=as.matrix(X) X=cbind(x0=1, X) Y=as.matrix(iris[, &quot;Petal.Length&quot;]) solve(t(X) %*% X) %*% t(X) %*% Y ##                    [,1] ## x0           -0.2627112 ## Petal.Width   1.4467934 ## Sepal.Length  0.7291384 ## Sepal.Width  -0.6460124 lm.fit ##  ## Call: ## lm(formula = Petal.Length ~ Petal.Width + Sepal.Length + Sepal.Width,  ##     data = iris) ##  ## Coefficients: ##  (Intercept)   Petal.Width  Sepal.Length   Sepal.Width   ##      -0.2627        1.4468        0.7291       -0.6460 按照公式计算出来，和lm.fit的结果是一样的。    "],
["general-linear-model.html", "9 General Linear Model 9.1 T检验是回归的特例 9.2 T检验是方差分析的特例 9.3 ANOVA是多重回归的特例", " 9 General Linear Model  9.1 T检验是回归的特例 data(iris) tt &lt;- iris[, c(&quot;Petal.Length&quot;, &quot;Sepal.Length&quot;)] t.test(Petal.Length, Sepal.Length, data=tt, var.equal=TRUE) ##  ##  Two Sample t-test ##  ## data:  Petal.Length and Sepal.Length ## t = -13.098, df = 298, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ##  -2.398643 -1.772023 ## sample estimates: ## mean of x mean of y  ##  3.758000  5.843333 我们取iris的Petal.Length和Sepal.Length做两样本T检验，因为方差分析和回归分析都假定齐方差，所以这里以齐方差方式进行T检验。 数据可以转换成以下分组形式： require(reshape2) ## Loading required package: reshape2 tt2 &lt;- melt(tt) ## No id variables; using all as measure variables head(tt2) ##       variable value ## 1 Petal.Length   1.4 ## 2 Petal.Length   1.4 ## 3 Petal.Length   1.3 ## 4 Petal.Length   1.5 ## 5 Petal.Length   1.4 ## 6 Petal.Length   1.7 t.test(value ~ variable, data=tt2, var.equal=TRUE) ##  ##  Two Sample t-test ##  ## data:  value by variable ## t = -13.098, df = 298, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ##  -2.398643 -1.772023 ## sample estimates: ## mean in group Petal.Length mean in group Sepal.Length  ##                   3.758000                   5.843333 分成两组，按分组变量进行T检验，结果是一样的。 对分组数据进行回归分析： lm.fit  &lt;- lm(value ~ variable, data=tt2) summary(lm.fit) ##  ## Call: ## lm(formula = value ~ variable, data = tt2) ##  ## Residuals: ##     Min      1Q  Median      3Q     Max  ## -2.7580 -0.8433  0.1993  0.9457  3.1420  ##  ## Coefficients: ##                      Estimate Std. Error t value Pr(&gt;|t|)     ## (Intercept)            3.7580     0.1126   33.38   &lt;2e-16 *** ## variableSepal.Length   2.0853     0.1592   13.10   &lt;2e-16 *** ## --- ## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ##  ## Residual standard error: 1.379 on 298 degrees of freedom ## Multiple R-squared:  0.3654, Adjusted R-squared:  0.3632  ## F-statistic: 171.6 on 1 and 298 DF,  p-value: &lt; 2.2e-16 回归分析完全包含了T检验，并且给出更丰富的信息。 Intercept相当于Petal.Length的均值，Slope则是均值差，slope的T检验结果和上面两样本均值差T检验是一样的。 而F值是T检验结果的平方： 13.10^2 ## [1] 171.61   9.2 T检验是方差分析的特例 res &lt;- aov(value ~ variable, data=tt2) summary(res) ##              Df Sum Sq Mean Sq F value Pr(&gt;F)     ## variable      1  326.1   326.1   171.6 &lt;2e-16 *** ## Residuals   298  566.5     1.9                    ## --- ## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 one way ANOVA进行两组分析和T检验是一样一样的，F值是t值的平方。 我们常说T检验和ANOVA是用来做组间比较的，而相关性和回归是用来度量相互关系的，实际上，他们是没有差别的。   9.3 ANOVA是多重回归的特例 这里使用two way ANOVA一节中使用的数据： data &lt;- read.table(&quot;data/gender_dose.tsv&quot;, header=TRUE) summary(with(data, aov(Alertness~Gender+Dosage))) ##             Df Sum Sq Mean Sq F value Pr(&gt;F)   ## Gender       1  76.56   76.56   3.197 0.0971 . ## Dosage       1   5.06    5.06   0.211 0.6533   ## Residuals   13 311.31   23.95                  ## --- ## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(lm(Alertness~Gender+Dosage, data=data)) ##  ## Call: ## lm(formula = Alertness ~ Gender + Dosage, data = data) ##  ## Residuals: ##    Min     1Q Median     3Q    Max  ## -6.438 -3.406  0.000  1.594 10.562  ##  ## Coefficients: ##             Estimate Std. Error t value Pr(&gt;|t|)     ## (Intercept)   15.688      2.119   7.403 5.17e-06 *** ## Genderm       -4.375      2.447  -1.788   0.0971 .   ## Dosageb        1.125      2.447   0.460   0.6533     ## --- ## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ##  ## Residual standard error: 4.894 on 13 degrees of freedom ## Multiple R-squared:  0.2077, Adjusted R-squared:  0.08584  ## F-statistic: 1.704 on 2 and 13 DF,  p-value: 0.2201 上面的例子中假设Gender和Dosage是完全无关的，如果把两因素的互作考虑在内，则： summary(with(data, aov(Alertness~Gender*Dosage))) ##               Df Sum Sq Mean Sq F value Pr(&gt;F) ## Gender         1  76.56   76.56   2.952  0.111 ## Dosage         1   5.06    5.06   0.195  0.666 ## Gender:Dosage  1   0.06    0.06   0.002  0.962 ## Residuals     12 311.25   25.94 summary(lm(Alertness~Gender+Dosage+Gender*Dosage, data=data)) ##  ## Call: ## lm(formula = Alertness ~ Gender + Dosage + Gender * Dosage, data = data) ##  ## Residuals: ##    Min     1Q Median     3Q    Max  ## -6.500 -3.375  0.000  1.562 10.500  ##  ## Coefficients: ##                 Estimate Std. Error t value Pr(&gt;|t|)     ## (Intercept)       15.750      2.546   6.185 4.69e-05 *** ## Genderm           -4.500      3.601  -1.250    0.235     ## Dosageb            1.000      3.601   0.278    0.786     ## Genderm:Dosageb    0.250      5.093   0.049    0.962     ## --- ## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ##  ## Residual standard error: 5.093 on 12 degrees of freedom ## Multiple R-squared:  0.2079, Adjusted R-squared:  0.009862  ## F-statistic:  1.05 on 3 and 12 DF,  p-value: 0.4062   "],
["logistic-regression.html", "10 Logistic Regression", " 10 Logistic Regression 线性回归是从X预测出Y，Y是数值型；而逻辑回归也是从X预测Y，但Y是二分变量。Fisher线性区分法通过寻找预测值的线性组合使得两组间的差异最大化，以此来预测分组的归属问题。这个方法要求预测变量必须是连续型，逻辑回归做为一个superior alternative，允许二分预测变量。 Y值是二分变量，以(0,1)表示，我们可以把它当成是二项式过程，p是1的比例，q=1-p是0的比例，1代表成功而0代表失败，逻辑曲线，预测值总是0,1之间。 首先把比例转换成odds，如果p是成功概率，那么支持成功的odds是： \\[ odds=\\frac{p}{q}=\\frac{p}{1-p}\\] 在逻辑回归中，我们使用的统计量叫分对数(logit)，它是odds的自然对数。 \\[ logit = ln(odds) = b_0+b_1x_1+b_2x_2+...+b_kx_k\\] 使用线性回归对logit进行预测， data(iris) dd=iris[iris$Species != &quot;setosa&quot;,] dd$Species=as.numeric(dd$Species)-2 res=glm(Species ~ Sepal.Length+ Sepal.Width+ Petal.Length + Petal.Width, data=dd, family=&quot;binomial&quot;) summary(res) ##  ## Call: ## glm(formula = Species ~ Sepal.Length + Sepal.Width + Petal.Length +  ##     Petal.Width, family = &quot;binomial&quot;, data = dd) ##  ## Deviance Residuals:  ##      Min        1Q    Median        3Q       Max   ## -2.01105  -0.00541  -0.00001   0.00677   1.78065   ##  ## Coefficients: ##              Estimate Std. Error z value Pr(&gt;|z|)   ## (Intercept)   -42.638     25.707  -1.659   0.0972 . ## Sepal.Length   -2.465      2.394  -1.030   0.3032   ## Sepal.Width    -6.681      4.480  -1.491   0.1359   ## Petal.Length    9.429      4.737   1.991   0.0465 * ## Petal.Width    18.286      9.743   1.877   0.0605 . ## --- ## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ##  ## (Dispersion parameter for binomial family taken to be 1) ##  ##     Null deviance: 138.629  on 99  degrees of freedom ## Residual deviance:  11.899  on 95  degrees of freedom ## AIC: 21.899 ##  ## Number of Fisher Scoring iterations: 10 结果使用的是Wald z statistic，分析显示只有Petal.Length是显著的。 res2=glm(Species ~ Petal.Length, data=dd, family=&quot;binomial&quot;) summary(res2) ##  ## Call: ## glm(formula = Species ~ Petal.Length, family = &quot;binomial&quot;, data = dd) ##  ## Deviance Residuals:  ##      Min        1Q    Median        3Q       Max   ## -2.11738  -0.12758  -0.00009   0.05865   2.57260   ##  ## Coefficients: ##              Estimate Std. Error z value Pr(&gt;|z|)     ## (Intercept)   -43.781     11.110  -3.941 8.12e-05 *** ## Petal.Length    9.002      2.283   3.943 8.04e-05 *** ## --- ## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ##  ## (Dispersion parameter for binomial family taken to be 1) ##  ##     Null deviance: 138.629  on 99  degrees of freedom ## Residual deviance:  33.432  on 98  degrees of freedom ## AIC: 37.432 ##  ## Number of Fisher Scoring iterations: 8 AIC(Akaike information criterion)用来度量模型拟合，它度量使用模型来描述Y变量信息损失的相对量，值越小，模型越好。 AIC可以用来做模型选择。像上面res和res2分别使用多个变量和单一变量来做拟合，AIC分别为21.9和37.43，显然第一个模型，使用多个变量来做拟合效果要好。 模型拟合可以使用卡方检验来检验拟合得好不好，检验的是null deviance (the null model)和fitted model的差别。 with(res, pchisq(null.deviance-deviance, df.null-df.residual, lower.tail=FALSE)) ## [1] 1.947107e-26 p值很小，证明模型是非常好的。 par(mfrow=c(2,2)) plot(res)  同样可以用plot画出各种图，来辅助诊断。 logit=log(res$fitted.values/(1-res$fitted.values)) plot(logit, res$fitted.values)  mean(round(res$fitted.values) == dd$Species) ## [1] 0.98 从数值上看，98%的数据归类是正确的。当然这个也可以使用卡方检验来检验： dd$fitted=round(res$fitted.values) chisq.test(with(dd, table(fitted, Species))) ##  ##  Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ##  ## data:  with(dd, table(fitted, Species)) ## X-squared = 88.36, df = 1, p-value &lt; 2.2e-16 逻辑回归也有很多alternative的方法，比如 \\(Hotelling&#39;s\\; T^2\\) 和Probit regression.  "],
["section-11.html", "11 生物实验统计分析 11.1 RT-PCR", " 11 生物实验统计分析  11.1 RT-PCR RT-PCR相对定量有两个模型，一是\\(\\Delta\\Delta{Ct}\\) 模型，一个是扩增效率校正模型。这里我们先讨论简单的模型：\\(\\Delta\\Delta{Ct}\\) 模型，在这一模型中，假定扩增效率为2，即每个PCR cycle，产物倍增，由以下公式给出： \\(Ratio = 2^{-\\Delta\\Delta{Ct}}\\) 其中\\(\\Delta\\Delta{Ct} = \\Delta{Ct}_{treated} - \\Delta{Ct}_{control}\\) , \\(\\Delta{Ct}_{treated}\\) 和\\(\\Delta{Ct}_{control}\\) 分别是treatment组和control组中目标基因和参照基因的Ct差，即 \\(\\Delta{Ct} = Ct_{target} - Ct_{reference}\\) . 扩增效率肯定是在(1,2)之间，理想状态下才是2，而理想状态通常是不存在的。这个模型的假设点其实是treatment组和control组的扩增效率一样，至于是不是2，比2小多少，并不影响后续的统计分析。  11.1.1 Data 我们来看以下一份数据，4组重复实验，用RT-PCR测了gene01和gene02的表达量，HK代表house keeping gene，即参照。以下数值为原始的Ct值。 ct &lt;- data.frame(             sample = rep(rep(1:4, each=3), 2),         treatment = rep(c(&quot;Control&quot;,&quot;Treated&quot;), each=12),         gene01 = c(23.22,23.34,23.12,24.06,24.15,24.15,23.18,23.13,                     23.10,24.78,24.45,24.67,23.11,22.99,23.10,22.77,                     22.99,23.06,23.73,24.01,23.80,23.73,23.83,23.73),         gene02 = c(29.08,29.04,29.39,28.23,28.01,28.12,28.79,28.43,                     28.49,31.37,30.74,31.09,27.11,27.24,27.37,25.52,                     25.72,25.52,27.43,26.73,26.65,27.96,27.84,27.98),         HK = c(19.68,19.69,19.80,19.95,19.93,19.97,19.93,20.02,20.27,                     19.93,19.88,19.90,20.61,19.98,20.57,19.68,19.95,                     19.85,20.27,20.08,20.07,20.10,20.07,20.10)             ) print(ct) ##    sample treatment gene01 gene02    HK ## 1       1   Control  23.22  29.08 19.68 ## 2       1   Control  23.34  29.04 19.69 ## 3       1   Control  23.12  29.39 19.80 ## 4       2   Control  24.06  28.23 19.95 ## 5       2   Control  24.15  28.01 19.93 ## 6       2   Control  24.15  28.12 19.97 ## 7       3   Control  23.18  28.79 19.93 ## 8       3   Control  23.13  28.43 20.02 ## 9       3   Control  23.10  28.49 20.27 ## 10      4   Control  24.78  31.37 19.93 ## 11      4   Control  24.45  30.74 19.88 ## 12      4   Control  24.67  31.09 19.90 ## 13      1   Treated  23.11  27.11 20.61 ## 14      1   Treated  22.99  27.24 19.98 ## 15      1   Treated  23.10  27.37 20.57 ## 16      2   Treated  22.77  25.52 19.68 ## 17      2   Treated  22.99  25.72 19.95 ## 18      2   Treated  23.06  25.52 19.85 ## 19      3   Treated  23.73  27.43 20.27 ## 20      3   Treated  24.01  26.73 20.08 ## 21      3   Treated  23.80  26.65 20.07 ## 22      4   Treated  23.73  27.96 20.10 ## 23      4   Treated  23.83  27.84 20.07 ## 24      4   Treated  23.73  27.98 20.10   11.1.2 Data clean 每个sample，测了三次技术重复，我们使用平均值来提高精度。 require(plyr) ct &lt;- ddply(ct, .(sample, treatment), function(x) data.frame(gene01=mean(x$gene01), gene02=mean(x$gene02), HK=mean(x$HK))) print(ct) ##   sample treatment   gene01   gene02       HK ## 1      1   Control 23.22667 29.17000 19.72333 ## 2      1   Treated 23.06667 27.24000 20.38667 ## 3      2   Control 24.12000 28.12000 19.95000 ## 4      2   Treated 22.94000 25.58667 19.82667 ## 5      3   Control 23.13667 28.57000 20.07333 ## 6      3   Treated 23.84667 26.93667 20.14000 ## 7      4   Control 24.63333 31.06667 19.90333 ## 8      4   Treated 23.76333 27.92667 20.09000   11.1.3 Calculate statistical metric 按照\\(\\Delta\\Delta{Ct}\\) 模型，我们先计算\\(\\Delta{Ct}\\) ，然后计算\\(\\Delta\\Delta{Ct}\\) ，最终计算出Ratio。 delta.ct &lt;- ddply(ct, .(sample, treatment), function(x) data.frame(gene01=x$gene01-x$HK, gene02=x$gene02-x$HK)) print(delta.ct) ##   sample treatment   gene01    gene02 ## 1      1   Control 3.503333  9.446667 ## 2      1   Treated 2.680000  6.853333 ## 3      2   Control 4.170000  8.170000 ## 4      2   Treated 3.113333  5.760000 ## 5      3   Control 3.063333  8.496667 ## 6      3   Treated 3.706667  6.796667 ## 7      4   Control 4.730000 11.163333 ## 8      4   Treated 3.673333  7.836667 ## Delta Delta Ct dd.ct &lt;- subset(delta.ct, treatment==&quot;Treated&quot;, select=c(gene01, gene02)) - subset(delta.ct, treatment==&quot;Control&quot;, select=c(gene01, gene02)) print(dd.ct) ##       gene01    gene02 ## 2 -0.8233333 -2.593333 ## 4 -1.0566667 -2.410000 ## 6  0.6433333 -1.700000 ## 8 -1.0566667 -3.326667 ratio &lt;- 2^(-1*dd.ct) print(ratio) ##     gene01    gene02 ## 2 1.769490  6.034915 ## 4 2.080120  5.314743 ## 6 0.640232  3.249010 ## 8 2.080120 10.032899 这个ratio值计算出，经过扩增后，目的基因在处理组中的产量是对照组的多少倍。   11.1.4 Statistical Analysis 计算出ratio后，很多人就拿它来做t检验。这种做法是不对的，因为ratio的分布明显是右偏的。从理论上看，PCR的扩增可以分成以下三个阶段，指数扩增、线性扩增、平台期。   theoretical plot  当试剂量足够的时候，就处于指数扩增阶段，只有当PCR试剂不足时，才进入线性扩增期，最后当试剂被耗尽时，产物不增长，位于平台期。   theoretical plot  在我们的实验中，通常PCR反应是处于指数增长期。我们将其进行对数处理，指数增长就会变成线性增长，这样数据的右偏现象就会有明显减弱，当然我不敢说对数转换后，它就能呈正态分布，但起码对数处理后数据的分布比原始ratio数据更接近正态了。  11.1.4.1 T Test 只有偏态不明显的情况下，才可以用t检验进行分析。 ## T test for gene01 t.test(log(ratio[,1], base=2), mu = 0) ##  ##  One Sample t-test ##  ## data:  log(ratio[, 1], base = 2) ## t = 1.4009, df = 3, p-value = 0.2558 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ##  -0.729139  1.875806 ## sample estimates: ## mean of x  ## 0.5733333 ## T test for gene02 t.test(log(ratio[,2], base=2), mu = 0) ##  ##  One Sample t-test ##  ## data:  log(ratio[, 2], base = 2) ## t = 7.5039, df = 3, p-value = 0.004904 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ##  1.44405 3.57095 ## sample estimates: ## mean of x  ##    2.5075  11.1.4.1.1 Use \\(-\\Delta\\Delta{Ct}\\) directly 如果我们要使用T检验，就得做对数处理，细心的人应该会发现ratio的计算就是指数运算，ratio取对数之后，就是\\(-\\Delta\\Delta{Ct}\\) ，所以我们可以直接使用它来做检验。 ## T test for gene02 t.test(-dd.ct[,2]) ##  ##  One Sample t-test ##  ## data:  -dd.ct[, 2] ## t = 7.5039, df = 3, p-value = 0.004904 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ##  1.44405 3.57095 ## sample estimates: ## mean of x  ##    2.5075    11.1.4.2 ANOVA 另一种方法，使用ANOVA进行统计，这也是参数统计方法，在我们这种小样本的情况下，同样对数据分布有严格的要求，我们不能基于ratio做统计，要基于\\(\\Delta{Ct}\\) 值。 print(delta.ct) ##   sample treatment   gene01    gene02 ## 1      1   Control 3.503333  9.446667 ## 2      1   Treated 2.680000  6.853333 ## 3      2   Control 4.170000  8.170000 ## 4      2   Treated 3.113333  5.760000 ## 5      3   Control 3.063333  8.496667 ## 6      3   Treated 3.706667  6.796667 ## 7      4   Control 4.730000 11.163333 ## 8      4   Treated 3.673333  7.836667 summary(aov(gene01 ~ treatment, data=delta.ct)) ##             Df Sum Sq Mean Sq F value Pr(&gt;F) ## treatment    1 0.6574  0.6574   1.687  0.242 ## Residuals    6 2.3385  0.3898 summary(aov(gene02 ~ treatment, data=delta.ct)) ##             Df Sum Sq Mean Sq F value Pr(&gt;F)   ## treatment    1 12.575  12.575   9.963 0.0197 * ## Residuals    6  7.573   1.262                  ## --- ## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ANOVA也是比较常用的，而且可以应用于多组实验和多个因素中。但是如果按照上面的代码来做，还不如用T test，因为实验是成对（tretated vs control）进行的，而这个信息在上面的ANOVA分析中，被扔了。 实际上 \\(-\\Delta\\Delta{Ct}\\) 的单样本T检验，相当于\\(\\Delta{Ct}\\) 的双样本T检验。 t.test(gene02 ~ treatment, delta.ct, paired=T) ##  ##  Paired t-test ##  ## data:  gene02 by treatment ## t = 7.5039, df = 3, p-value = 0.004904 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ##  1.44405 3.57095 ## sample estimates: ## mean of the differences  ##                  2.5075 而这里用ANOVA，没有用到成对的信息，计算出来的p值和unpaired T test就基本相等。 t.test(gene01 ~ treatment, delta.ct, paired=F) ##  ##  Welch Two Sample t-test ##  ## data:  gene01 by treatment ## t = 1.2988, df = 5.2396, p-value = 0.2482 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ##  -0.5460188  1.6926855 ## sample estimates: ## mean in group Control mean in group Treated  ##              3.866667              3.293333 t.test(gene02 ~ treatment, delta.ct, paired=F) ##  ##  Welch Two Sample t-test ##  ## data:  gene02 by treatment ## t = 3.1565, df = 5.064, p-value = 0.02476 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ##  0.473167 4.541833 ## sample estimates: ## mean in group Control mean in group Treated  ##              9.319167              6.811667 做为成对T检验的扩展，使用方差分析进行多组实验的比较，需要用到的是重复测量方差分析（repeated-measures ANOVA）。 delta.ct[,1] = factor(delta.ct[,1]) delta.ct[,2] = factor(delta.ct[,2]) summary(aov(gene01 ~ treatment+Error(sample/treatment), data=delta.ct)) ##  ## Error: sample ##           Df Sum Sq Mean Sq F value Pr(&gt;F) ## Residuals  3  1.333  0.4445                ##  ## Error: sample:treatment ##           Df Sum Sq Mean Sq F value Pr(&gt;F) ## treatment  1 0.6574  0.6574   1.962  0.256 ## Residuals  3 1.0050  0.3350 summary(aov(gene02 ~ treatment+Error(sample/treatment), data=delta.ct)) ##  ## Error: sample ##           Df Sum Sq Mean Sq F value Pr(&gt;F) ## Residuals  3  6.903   2.301                ##  ## Error: sample:treatment ##           Df Sum Sq Mean Sq F value Pr(&gt;F)    ## treatment  1  12.57  12.575   56.31 0.0049 ** ## Residuals  3   0.67   0.223                   ## --- ## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 这样算出来的p值就和成对T检验几乎一样。   11.1.4.3 Wilcoxon Rank Sum Test 我们计算出来的ratio，是经过PCR扩增之后产物的比值，这是经过指数扩增的，而如果我们取对数，相当于消除了指数增长的效果，\\(-\\Delta\\Delta{Ct}\\) 其实就是原始量的差值。 所以ratio的比较，相当于是扩增后的比较，而\\(\\log_2 {ratio}\\) 的比较，则相当于是扩增之前的比较。这两种数据当然都是可以拿来做统计分析的，只不过T test等参数检验方法对数据分布有要求，不适合于ratio值的统计分析而已。如果我们使用非参数检验，则不管是ratio值也好，\\(\\log_2 {ratio}\\) 值也好，结果都是一样的。非参数检验并不使用数据的原始值，而是使用其排序值来做检验，我们并不能确定对数处理后的数据就呈正态分布，还有就是通常我们的实验重复次数是非常少的，在这种情况下，使用非参数检验，其实也是一种保守的、不容易犯错的方法。 ## Wilcoxon Rank Sum test for gene02, original data wilcox.test( ratio[,2], mu=1) ##  ##  Wilcoxon signed rank test ##  ## data:  ratio[, 2] ## V = 10, p-value = 0.125 ## alternative hypothesis: true location is not equal to 1 ## Wilcoxon Rank Sum test for gene02, log2 transformed data wilcox.test( log(ratio[,2], base=2), mu=0) ##  ##  Wilcoxon signed rank test ##  ## data:  log(ratio[, 2], base = 2) ## V = 10, p-value = 0.125 ## alternative hypothesis: true location is not equal to 0 说它不容易犯错，是因为它对数据分布没要求；说它保守，是因为在计算p-value上，灵敏度没有T test这么高，p-value会大一些。   11.1.4.4 Permutation Test 生物学的数据通常比较messy，non-normal，重复次数又少。在我们不能确定数据分布是否正态的情况下，使用参数检验，很可能结果bias太大，使用非参数检验，又觉得power不够大，那么我们可以使用permutation test。这不失为一种好方法，通过随机变换两组数据的标签，观察它们的均值差，我们将得到均值差的分布，将我们实际的测量值对比于随机得到的差值分布，算出随机得到的差值大于实际的测量值的概率。 perm.test &lt;- function(d1, d2, nIter=10000) {     if (length(d2) == 1)         d2 &lt;- rep(d2, length(d1))     m &lt;- mean(d2-d1)     pooledData &lt;- c(d1, d2)     n &lt;- length(d1)     meanDiff &lt;- numeric(nIter)     for (i in 1:nIter) {         idx &lt;- sample(1:length(pooledData), n, replace=FALSE)         d1 &lt;- pooledData[idx]         d2 &lt;- pooledData[-idx]         meanDiff[i] &lt;- mean(d2) - mean(d1)     }     p &lt;- mean(abs(meanDiff) &gt;= abs(m))     return(p) }  perm.test(ratio[,1],1) ## [1] 0.1443 perm.test(-dd.ct[,1], 0) ## [1] 0.1414 perm.test(ratio[,2],1) ## [1] 0.0286 perm.test(-dd.ct[,2], 0) ## [1] 0.0285 从结果上看，用ratio还是用\\(-\\Delta\\Delta{Ct}\\) 差不多，这表明原始数据的分布完全不关事。  没有吃没有穿 自有那敌人送上前 没有枪没有炮 敌人给我们造                      ----游击队之歌 这个方法的奇妙之处，就在于通过bootstrap，产生针对我们统计量的一个背景分布，再由这个背景分布计算出p-value。    11.1.5 Plot 至于画图，我推荐使用\\(-\\Delta\\Delta{Ct}\\) 来画，这个CONTROL是0，不用画。如果要用ratio来画，你觉得CONTROL每次都是画一个mean=1，sd=0的柱子，有意义吗？ anyway，我下面的画图代码，同样也适用于拿来画ratio值。 BTW：这里图中标的p值，使用的是T检验算出来的p值。 require(reshape2) dd.ctm &lt;- melt(-dd.ct) ## No id variables; using all as measure variables print(dd.ctm) ##   variable      value ## 1   gene01  0.8233333 ## 2   gene01  1.0566667 ## 3   gene01 -0.6433333 ## 4   gene01  1.0566667 ## 5   gene02  2.5933333 ## 6   gene02  2.4100000 ## 7   gene02  1.7000000 ## 8   gene02  3.3266667 dd.cts &lt;- ddply(dd.ctm, .(variable), function(x) data.frame(mean=mean(x$value), sd=sd(x$value))) print(dd.cts) ##   variable      mean        sd ## 1   gene01 0.5733333 0.8185353 ## 2   gene02 2.5075000 0.6683222 require(ggplot2) p &lt;- ggplot(dd.cts, aes(variable, mean, fill=variable, width=.5))+geom_bar(stat=&quot;identity&quot;, position=position_dodge(width=.8), colour=&quot;black&quot;)+geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2, position=position_dodge(width=.8)) p &lt;- p+scale_fill_manual(values=c(&quot;white&quot;, &quot;grey80&quot;)) p &lt;- p+theme_bw()+theme(legend.position=&quot;none&quot;) + xlab(&quot;&quot;)+ylab(expression(paste(&quot;-&quot;, Delta, Delta, &quot;Ct&quot;))) p &lt;- p+theme(axis.text.x=element_text(face=&quot;bold&quot;, size=14), axis.text.y=element_text(face=&quot;bold&quot;, size=14), axis.title.y=element_text(size=18, face=&quot;bold&quot;)) p &lt;- p+annotate(&quot;text&quot;, x=2,y=3.5, label=&quot;p &lt; 0.005\\n**&quot;, size=4) print(p)  再来一个箱式图，我其实觉得箱式图比柱状图要好一些，这里我除了用箱式图画数据分布之后，把原始的数据点也给画了。 p2 &lt;- ggplot(dd.ctm, aes(variable, value, fill=variable, colour=variable)) p2 &lt;- p2+geom_boxplot(alpha=.3, width=.5)+geom_dotplot(binaxis=&#39;y&#39;, stackdir=&#39;center&#39;, dotsize=.5) p2 &lt;- p2+theme_bw()+theme(legend.position=&quot;none&quot;) + xlab(&quot;&quot;)+ylab(expression(paste(&quot;-&quot;, Delta, Delta, &quot;Ct&quot;))) p2 &lt;- p2+theme(axis.text.x=element_text(face=&quot;bold&quot;, size=14), axis.text.y=element_text(face=&quot;bold&quot;, size=14), axis.title.y=element_text(size=18, face=&quot;bold&quot;)) p2 &lt;- p2+annotate(&quot;text&quot;, x=2,y=3.5, label=&quot;p &lt; 0.005\\n**&quot;, size=4) print(p2) ## `stat_bindot()` using `bins = 30`. Pick better value with `binwidth`.    11.1.6 Summary 据我的观察，生物坑里大家对T检验特别有爱，我甚至看到用T检验去分析免疫组化数据的，而且还发在Nature子刊上，我都吐槽无力了。如果你喜欢T检验，你要注意数据分布，那么在分析RT-PCR数据上，请用\\(-\\Delta\\Delta{Ct}\\) 作为统计量，或者将你算完的ratio值取对数也是一样的。用ANOVA时，也必须同样注意。 在这个重复次数少，数据又非常messy的生物世界里，保守点的做法是使用非参数统计，即使你使用ratio这种严重右偏的数据，你的统计分析照样没问题，多学点总是好的，凡事不能太依赖于T检验，因为在很多其它的场景里，正如这里的ratio值，你不能简单套用T检验。 Permutation test其实才是工具箱里不可或缺的一件，应用场景和非参一样广，同时灵敏度又比非参高，你值得拥有。    "]
]
